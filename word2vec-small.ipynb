{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "As described in [A Word2Vec Keras tutorial](https://adventuresinmachinelearning.com/word2vec-keras-tutorial/)\n",
    "\n",
    "\n",
    "![](https://i0.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2017/08/Negative-sampling-architecture-1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Lambda, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"ARGHHH error\")\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=JILfse0prtw\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    #filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(\"data1.zip\")\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on above\n",
    "\n",
    "#####  `data` is a vector of integers representing the text.\n",
    "For example\n",
    "\n",
    "    data[:10]\n",
    "    [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156]\n",
    "    \n",
    "##### `count`     is an array of word count tuples\n",
    "Where the count is the number of times that word appeared in the text\n",
    "\n",
    "    [['UNK', 1737307],\n",
    "     ('the', 1061396),\n",
    "     ('of', 593677),\n",
    "     ('and', 416629),\n",
    "     ('one', 411764),\n",
    "     ('in', 372201),\n",
    "     \n",
    "##### `dictionary` is the mapping (a Python dictionary) from words to their associated integers\n",
    "\n",
    "    {'UNK': 0,\n",
    "     'the': 1,\n",
    "     'of': 2,\n",
    "     'and': 3,\n",
    "     'one': 4,\n",
    "     'in': 5,\n",
    "     'a': 6,\n",
    "     \n",
    "     \n",
    "##### `reverse_dictionary` is the mapping (a Python dictionary) from integers to their associated words\n",
    "\n",
    "     reverse_dictionary[5234]\n",
    "     'anarchism'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'lazy', 'old', 'dog', 'sat', 'on', 'the']\n",
      "[1, 2, 3, 11, 4, 5, 1]\n",
      "Making skipgrams\n",
      "Skipgrams complete\n",
      "[[2, 3], [16, 3], [16, 7], [9, 1], [16, 20], [19, 4], [16, 4], [9, 9], [9, 1], [9, 10]] [1, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])\n",
    "\n",
    "window_size = 1\n",
    "vector_dim = 4\n",
    "epochs = 200000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "print(\"Making skipgrams\")\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "print(\"Skipgrams complete\")\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lazy old 1\n",
      "burro old 1\n",
      "burro chair. 0\n",
      "mat. the 1\n",
      "burro elephant 0\n",
      "bear sat 1\n",
      "burro sat 1\n",
      "mat. mat. 0\n",
      "mat. the 1\n",
      "mat. floor. 0\n",
      "bear old 1\n",
      "lazy burro 0\n",
      "bear pig 0\n",
      "lazy pig 0\n",
      "lazy the 1\n",
      "bear on 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(couples)\n",
    "def printPair(e):\n",
    "    pair = e[0]\n",
    "    label = e[1]\n",
    "    print(reverse_dictionary[pair[0]], reverse_dictionary[pair[1]], label)\n",
    "    \n",
    "for couple in zip(couples, labels):    \n",
    "    printPair(couple)\n",
    "#for w in data:\n",
    "#    print(reverse_dictionary[w])\n",
    "max(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.layers\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Cosine Similarity Operation\n",
    " which will be output in a secondary model\n",
    " \n",
    " ```similarity = keras.layers.merge([target, context], mode='cos', dot_axes=0)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "#similarity = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([target, context])\n",
    "similarity =  dot([target, context], 1, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "  import sys\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context],  1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity =  dot([target, context], 1, normalize=True)\n",
    "\n",
    "\n",
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "    \n",
    "    def get_sim(self, valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "    \n",
    "      \n",
    "    \n",
    "    \n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6917778253555298\n",
      "Iteration 5, loss=0.6975952386856079\n",
      "Iteration 10, loss=0.6879696846008301\n",
      "Iteration 15, loss=0.6968216300010681\n",
      "Iteration 20, loss=0.6965695023536682\n",
      "Iteration 25, loss=0.6908119320869446\n",
      "Iteration 30, loss=0.6966639757156372\n",
      "Iteration 35, loss=0.6891825199127197\n",
      "Iteration 40, loss=0.6891002058982849\n",
      "Iteration 45, loss=0.6908366680145264\n",
      "Iteration 50, loss=0.68485426902771\n",
      "Iteration 55, loss=0.6903255581855774\n",
      "Iteration 60, loss=0.6906092166900635\n",
      "Iteration 65, loss=0.6833970546722412\n",
      "Iteration 70, loss=0.6859609484672546\n",
      "Iteration 75, loss=0.6950205564498901\n",
      "Iteration 80, loss=0.6930654048919678\n",
      "Iteration 85, loss=0.6875976920127869\n",
      "Iteration 90, loss=0.6858574151992798\n",
      "Iteration 95, loss=0.693886399269104\n",
      "Iteration 100, loss=0.6885250210762024\n",
      "Iteration 105, loss=0.6875311732292175\n",
      "Iteration 110, loss=0.6866088509559631\n",
      "Iteration 115, loss=0.6850278377532959\n",
      "Iteration 120, loss=0.6863799095153809\n",
      "Iteration 125, loss=0.682975172996521\n",
      "Iteration 130, loss=0.6878942251205444\n",
      "Iteration 135, loss=0.6936937570571899\n",
      "Iteration 140, loss=0.6783322095870972\n",
      "Iteration 145, loss=0.6883935332298279\n",
      "Iteration 150, loss=0.6861173510551453\n",
      "Iteration 155, loss=0.6902397871017456\n",
      "Iteration 160, loss=0.6878073215484619\n",
      "Iteration 165, loss=0.6918656826019287\n",
      "Iteration 170, loss=0.6851670145988464\n",
      "Iteration 175, loss=0.6830363869667053\n",
      "Iteration 180, loss=0.6591242551803589\n",
      "Iteration 185, loss=0.6809120774269104\n",
      "Iteration 190, loss=0.6933742761611938\n",
      "Iteration 195, loss=0.676444947719574\n",
      "Iteration 200, loss=0.6707891225814819\n",
      "Iteration 205, loss=0.6663421988487244\n",
      "Iteration 210, loss=0.6791670322418213\n",
      "Iteration 215, loss=0.6821684241294861\n",
      "Iteration 220, loss=0.6758115291595459\n",
      "Iteration 225, loss=0.6806129813194275\n",
      "Iteration 230, loss=0.6611434817314148\n",
      "Iteration 235, loss=0.6806949973106384\n",
      "Iteration 240, loss=0.6780608892440796\n",
      "Iteration 245, loss=0.6567965149879456\n",
      "Iteration 250, loss=0.6733927130699158\n",
      "Iteration 255, loss=0.6753990054130554\n",
      "Iteration 260, loss=0.6757808923721313\n",
      "Iteration 265, loss=0.6824445724487305\n",
      "Iteration 270, loss=0.6838488578796387\n",
      "Iteration 275, loss=0.6697919964790344\n",
      "Iteration 280, loss=0.6495362520217896\n",
      "Iteration 285, loss=0.6821025013923645\n",
      "Iteration 290, loss=0.6435056328773499\n",
      "Iteration 295, loss=0.6769664883613586\n",
      "Iteration 300, loss=0.6291322708129883\n",
      "Iteration 305, loss=0.6374053359031677\n",
      "Iteration 310, loss=0.6346700191497803\n",
      "Iteration 315, loss=0.6244938373565674\n",
      "Iteration 320, loss=0.6749475598335266\n",
      "Iteration 325, loss=0.616506814956665\n",
      "Iteration 330, loss=0.6708469986915588\n",
      "Iteration 335, loss=0.6293671131134033\n",
      "Iteration 340, loss=0.6695176362991333\n",
      "Iteration 345, loss=0.6685358881950378\n",
      "Iteration 350, loss=0.658858597278595\n",
      "Iteration 355, loss=0.6694558262825012\n",
      "Iteration 360, loss=0.6695430874824524\n",
      "Iteration 365, loss=0.655160129070282\n",
      "Iteration 370, loss=0.6684008836746216\n",
      "Iteration 375, loss=0.6638673543930054\n",
      "Iteration 380, loss=0.6172125935554504\n",
      "Iteration 385, loss=0.663707435131073\n",
      "Iteration 390, loss=0.6618388295173645\n",
      "Iteration 395, loss=0.6100202202796936\n",
      "Iteration 400, loss=0.6496312618255615\n",
      "Iteration 405, loss=0.6039348244667053\n",
      "Iteration 410, loss=0.6328970193862915\n",
      "Iteration 415, loss=0.6472150683403015\n",
      "Iteration 420, loss=0.6565068960189819\n",
      "Iteration 425, loss=0.6528041958808899\n",
      "Iteration 430, loss=0.6489505767822266\n",
      "Iteration 435, loss=0.6328822374343872\n",
      "Iteration 440, loss=0.6508196592330933\n",
      "Iteration 445, loss=0.5970869660377502\n",
      "Iteration 450, loss=0.6225730776786804\n",
      "Iteration 455, loss=0.617243766784668\n",
      "Iteration 460, loss=0.5610716938972473\n",
      "Iteration 465, loss=0.6147602796554565\n",
      "Iteration 470, loss=0.632315993309021\n",
      "Iteration 475, loss=0.6117027401924133\n",
      "Iteration 480, loss=0.6095296740531921\n",
      "Iteration 485, loss=0.6163033843040466\n",
      "Iteration 490, loss=0.633401095867157\n",
      "Iteration 495, loss=0.5408591032028198\n",
      "Iteration 500, loss=0.5742121934890747\n",
      "Iteration 505, loss=0.6090587973594666\n",
      "Iteration 510, loss=0.5688866972923279\n",
      "Iteration 515, loss=0.6018433570861816\n",
      "Iteration 520, loss=0.6027109026908875\n",
      "Iteration 525, loss=0.6212881207466125\n",
      "Iteration 530, loss=0.6383311748504639\n",
      "Iteration 535, loss=0.6252548098564148\n",
      "Iteration 540, loss=0.5542656779289246\n",
      "Iteration 545, loss=0.6108751893043518\n",
      "Iteration 550, loss=0.5460139513015747\n",
      "Iteration 555, loss=0.6302916407585144\n",
      "Iteration 560, loss=0.5791454911231995\n",
      "Iteration 565, loss=0.6165593862533569\n",
      "Iteration 570, loss=0.578728199005127\n",
      "Iteration 575, loss=0.5317795276641846\n",
      "Iteration 580, loss=0.6008139848709106\n",
      "Iteration 585, loss=0.6020774245262146\n",
      "Iteration 590, loss=0.5227718353271484\n",
      "Iteration 595, loss=0.5939842462539673\n",
      "Iteration 600, loss=0.5882760882377625\n",
      "Iteration 605, loss=0.5116633176803589\n",
      "Iteration 610, loss=0.5490398406982422\n",
      "Iteration 615, loss=0.5807885527610779\n",
      "Iteration 620, loss=0.5025673508644104\n",
      "Iteration 625, loss=0.5891831517219543\n",
      "Iteration 630, loss=0.5341198444366455\n",
      "Iteration 635, loss=0.49506646394729614\n",
      "Iteration 640, loss=0.5778777599334717\n",
      "Iteration 645, loss=0.5784584283828735\n",
      "Iteration 650, loss=0.48428529500961304\n",
      "Iteration 655, loss=0.5891388058662415\n",
      "Iteration 660, loss=0.42386192083358765\n",
      "Iteration 665, loss=0.5281849503517151\n",
      "Iteration 670, loss=0.5775933861732483\n",
      "Iteration 675, loss=0.5706804990768433\n",
      "Iteration 680, loss=0.4048019349575043\n",
      "Iteration 685, loss=0.4002739489078522\n",
      "Iteration 690, loss=0.4983987808227539\n",
      "Iteration 695, loss=0.3880232572555542\n",
      "Iteration 700, loss=0.6016273498535156\n",
      "Iteration 705, loss=0.5427699685096741\n",
      "Iteration 710, loss=0.44049495458602905\n",
      "Iteration 715, loss=0.5354613065719604\n",
      "Iteration 720, loss=0.5496340394020081\n",
      "Iteration 725, loss=0.47867435216903687\n",
      "Iteration 730, loss=0.5440210103988647\n",
      "Iteration 735, loss=0.42403972148895264\n",
      "Iteration 740, loss=0.5431363582611084\n",
      "Iteration 745, loss=0.5371099710464478\n",
      "Iteration 750, loss=0.5350283980369568\n",
      "Iteration 755, loss=0.34975287318229675\n",
      "Iteration 760, loss=0.5272200107574463\n",
      "Iteration 765, loss=0.5518811345100403\n",
      "Iteration 770, loss=0.5215399265289307\n",
      "Iteration 775, loss=0.5317201614379883\n",
      "Iteration 780, loss=0.39513710141181946\n",
      "Iteration 785, loss=0.46086063981056213\n",
      "Iteration 790, loss=0.4194185435771942\n",
      "Iteration 795, loss=0.5242947340011597\n",
      "Iteration 800, loss=0.37940695881843567\n",
      "Iteration 805, loss=0.5337761044502258\n",
      "Iteration 810, loss=0.5083433389663696\n",
      "Iteration 815, loss=0.5022177696228027\n",
      "Iteration 820, loss=0.3681401312351227\n",
      "Iteration 825, loss=0.49812251329421997\n",
      "Iteration 830, loss=0.4905132055282593\n",
      "Iteration 835, loss=0.48245441913604736\n",
      "Iteration 840, loss=0.5028403997421265\n",
      "Iteration 845, loss=0.4763670861721039\n",
      "Iteration 850, loss=0.5366761088371277\n",
      "Iteration 855, loss=0.3461455702781677\n",
      "Iteration 860, loss=0.46698707342147827\n",
      "Iteration 865, loss=0.4770529866218567\n",
      "Iteration 870, loss=0.3345874547958374\n",
      "Iteration 875, loss=0.4353198707103729\n",
      "Iteration 880, loss=0.3610122799873352\n",
      "Iteration 885, loss=0.35524994134902954\n",
      "Iteration 890, loss=0.5204826593399048\n",
      "Iteration 895, loss=0.339249849319458\n",
      "Iteration 900, loss=0.24548378586769104\n",
      "Iteration 905, loss=0.3683094382286072\n",
      "Iteration 910, loss=0.37998852133750916\n",
      "Iteration 915, loss=0.30353307723999023\n",
      "Iteration 920, loss=0.4407375752925873\n",
      "Iteration 925, loss=0.2984890937805176\n",
      "Iteration 930, loss=0.22651265561580658\n",
      "Iteration 935, loss=0.43615445494651794\n",
      "Iteration 940, loss=0.3391065299510956\n",
      "Iteration 945, loss=0.4526905417442322\n",
      "Iteration 950, loss=0.3291667401790619\n",
      "Iteration 955, loss=0.2942824959754944\n",
      "Iteration 960, loss=0.2750542163848877\n",
      "Iteration 965, loss=0.26893898844718933\n",
      "Iteration 970, loss=0.19414469599723816\n",
      "Iteration 975, loss=0.27636465430259705\n",
      "Iteration 980, loss=0.2699263393878937\n",
      "Iteration 985, loss=0.3821975886821747\n",
      "Iteration 990, loss=0.3260106146335602\n",
      "Iteration 995, loss=0.3974687159061432\n",
      "Iteration 1000, loss=0.18392272293567657\n",
      "Iteration 1005, loss=0.2425166517496109\n",
      "Iteration 1010, loss=0.4474257230758667\n",
      "Iteration 1015, loss=0.23389562964439392\n",
      "Iteration 1020, loss=0.37960007786750793\n",
      "Iteration 1025, loss=0.16784757375717163\n",
      "Iteration 1030, loss=0.4310896694660187\n",
      "Iteration 1035, loss=0.41871389746665955\n",
      "Iteration 1040, loss=0.36704567074775696\n",
      "Iteration 1045, loss=0.21895131468772888\n",
      "Iteration 1050, loss=0.27614110708236694\n",
      "Iteration 1055, loss=0.43514755368232727\n",
      "Iteration 1060, loss=0.25025227665901184\n",
      "Iteration 1065, loss=0.412384033203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1070, loss=0.3674347400665283\n",
      "Iteration 1075, loss=0.4104231595993042\n",
      "Iteration 1080, loss=0.28318876028060913\n",
      "Iteration 1085, loss=0.20010925829410553\n",
      "Iteration 1090, loss=0.38863763213157654\n",
      "Iteration 1095, loss=0.19473513960838318\n",
      "Iteration 1100, loss=0.18988005816936493\n",
      "Iteration 1105, loss=0.2636026442050934\n",
      "Iteration 1110, loss=0.34390079975128174\n",
      "Iteration 1115, loss=0.33191201090812683\n",
      "Iteration 1120, loss=0.18064498901367188\n",
      "Iteration 1125, loss=0.38855165243148804\n",
      "Iteration 1130, loss=0.3719998896121979\n",
      "Iteration 1135, loss=0.10916591435670853\n",
      "Iteration 1140, loss=0.10648361593484879\n",
      "Iteration 1145, loss=0.2628527283668518\n",
      "Iteration 1150, loss=0.30662405490875244\n",
      "Iteration 1155, loss=0.31286007165908813\n",
      "Iteration 1160, loss=0.3689816892147064\n",
      "Iteration 1165, loss=0.22294072806835175\n",
      "Iteration 1170, loss=0.09709243476390839\n",
      "Iteration 1175, loss=0.30207955837249756\n",
      "Iteration 1180, loss=0.34604397416114807\n",
      "Iteration 1185, loss=0.23752671480178833\n",
      "Iteration 1190, loss=0.16078734397888184\n",
      "Iteration 1195, loss=0.3355795443058014\n",
      "Iteration 1200, loss=0.33654579520225525\n",
      "Iteration 1205, loss=0.27494460344314575\n",
      "Iteration 1210, loss=0.3052592873573303\n",
      "Iteration 1215, loss=0.2705656588077545\n",
      "Iteration 1220, loss=0.32175689935684204\n",
      "Iteration 1225, loss=0.14059165120124817\n",
      "Iteration 1230, loss=0.10271647572517395\n",
      "Iteration 1235, loss=0.16687828302383423\n",
      "Iteration 1240, loss=0.07285203784704208\n",
      "Iteration 1245, loss=0.0704074501991272\n",
      "Iteration 1250, loss=0.06796422600746155\n",
      "Iteration 1255, loss=0.19363845884799957\n",
      "Iteration 1260, loss=0.12359125912189484\n",
      "Iteration 1265, loss=0.2824118733406067\n",
      "Iteration 1270, loss=0.23479317128658295\n",
      "Iteration 1275, loss=0.11720224469900131\n",
      "Iteration 1280, loss=0.22717565298080444\n",
      "Iteration 1285, loss=0.058345597237348557\n",
      "Iteration 1290, loss=0.05672910064458847\n",
      "Iteration 1295, loss=0.11072956770658493\n",
      "Iteration 1300, loss=0.10892922431230545\n",
      "Iteration 1305, loss=0.2936105728149414\n",
      "Iteration 1310, loss=0.1250523030757904\n",
      "Iteration 1315, loss=0.254176527261734\n",
      "Iteration 1320, loss=0.13684256374835968\n",
      "Iteration 1325, loss=0.04612300917506218\n",
      "Iteration 1330, loss=0.11486615240573883\n",
      "Iteration 1335, loss=0.2796383500099182\n",
      "Iteration 1340, loss=0.09380476176738739\n",
      "Iteration 1345, loss=0.24756498634815216\n",
      "Iteration 1350, loss=0.2635285258293152\n",
      "Iteration 1355, loss=0.08770497888326645\n",
      "Iteration 1360, loss=0.11988159269094467\n",
      "Iteration 1365, loss=0.18007390201091766\n",
      "Iteration 1370, loss=0.050521671772003174\n",
      "Iteration 1375, loss=0.2174357920885086\n",
      "Iteration 1380, loss=0.11155974864959717\n",
      "Iteration 1385, loss=0.09026489406824112\n",
      "Iteration 1390, loss=0.12916199862957\n",
      "Iteration 1395, loss=0.1658603698015213\n",
      "Iteration 1400, loss=0.07398020476102829\n",
      "Iteration 1405, loss=0.02725428156554699\n",
      "Iteration 1410, loss=0.07947829365730286\n",
      "Iteration 1415, loss=0.06806955486536026\n",
      "Iteration 1420, loss=0.14566782116889954\n",
      "Iteration 1425, loss=0.2207990288734436\n",
      "Iteration 1430, loss=0.1477927565574646\n",
      "Iteration 1435, loss=0.05597558617591858\n",
      "Iteration 1440, loss=0.13898774981498718\n",
      "Iteration 1445, loss=0.1407158076763153\n",
      "Iteration 1450, loss=0.13026626408100128\n",
      "Iteration 1455, loss=0.15390963852405548\n",
      "Iteration 1460, loss=0.2087867110967636\n",
      "Iteration 1465, loss=0.09863591194152832\n",
      "Iteration 1470, loss=0.2002301812171936\n",
      "Iteration 1475, loss=0.09495191276073456\n",
      "Iteration 1480, loss=0.11806405335664749\n",
      "Iteration 1485, loss=0.16134847700595856\n",
      "Iteration 1490, loss=0.05109483003616333\n",
      "Iteration 1495, loss=0.049927111715078354\n",
      "Iteration 1500, loss=0.10916102677583694\n",
      "Iteration 1505, loss=0.18639802932739258\n",
      "Iteration 1510, loss=0.18172134459018707\n",
      "Iteration 1515, loss=0.09787868708372116\n",
      "Iteration 1520, loss=0.0962868332862854\n",
      "Iteration 1525, loss=0.03975570574402809\n",
      "Iteration 1530, loss=0.17550833523273468\n",
      "Iteration 1535, loss=0.13788366317749023\n",
      "Iteration 1540, loss=0.1267620325088501\n",
      "Iteration 1545, loss=0.0841994658112526\n",
      "Iteration 1550, loss=0.0399966724216938\n",
      "Iteration 1555, loss=0.01673009991645813\n",
      "Iteration 1560, loss=0.03742234408855438\n",
      "Iteration 1565, loss=0.1293903887271881\n",
      "Iteration 1570, loss=0.07799381017684937\n",
      "Iteration 1575, loss=0.09438035637140274\n",
      "Iteration 1580, loss=0.110320545732975\n",
      "Iteration 1585, loss=0.12075436115264893\n",
      "Iteration 1590, loss=0.07214082777500153\n",
      "Iteration 1595, loss=0.03335132077336311\n",
      "Iteration 1600, loss=0.05925646051764488\n",
      "Iteration 1605, loss=0.011926348321139812\n",
      "Iteration 1610, loss=0.1476503312587738\n",
      "Iteration 1615, loss=0.02154366485774517\n",
      "Iteration 1620, loss=0.02975771576166153\n",
      "Iteration 1625, loss=0.034840233623981476\n",
      "Iteration 1630, loss=0.027823403477668762\n",
      "Iteration 1635, loss=0.0057944487780332565\n",
      "Iteration 1640, loss=0.07094384729862213\n",
      "Iteration 1645, loss=0.02514856867492199\n",
      "Iteration 1650, loss=0.030094891786575317\n",
      "Iteration 1655, loss=0.06464321166276932\n",
      "Iteration 1660, loss=0.06296176463365555\n",
      "Iteration 1665, loss=0.017461277544498444\n",
      "Iteration 1670, loss=0.059881825000047684\n",
      "Iteration 1675, loss=0.08076059818267822\n",
      "Iteration 1680, loss=0.020441483706235886\n",
      "Iteration 1685, loss=0.020095067098736763\n",
      "Iteration 1690, loss=0.11891639232635498\n",
      "Iteration 1695, loss=0.023325501009821892\n",
      "Iteration 1700, loss=0.003331419313326478\n",
      "Iteration 1705, loss=0.05040671303868294\n",
      "Iteration 1710, loss=0.021539445966482162\n",
      "Iteration 1715, loss=0.002989464672282338\n",
      "Iteration 1720, loss=0.11042968183755875\n",
      "Iteration 1725, loss=0.0457659587264061\n",
      "Iteration 1730, loss=0.019725006073713303\n",
      "Iteration 1735, loss=0.0439787395298481\n",
      "Iteration 1740, loss=0.03411715105175972\n",
      "Iteration 1745, loss=0.011195505037903786\n",
      "Iteration 1750, loss=0.07444345951080322\n",
      "Iteration 1755, loss=0.03309711441397667\n",
      "Iteration 1760, loss=0.040163055062294006\n",
      "Iteration 1765, loss=0.029258571565151215\n",
      "Iteration 1770, loss=0.014581160619854927\n",
      "Iteration 1775, loss=0.05645041912794113\n",
      "Iteration 1780, loss=0.0036977045238018036\n",
      "Iteration 1785, loss=0.01269838772714138\n",
      "Iteration 1790, loss=0.09085427224636078\n",
      "Iteration 1795, loss=0.008113967254757881\n",
      "Iteration 1800, loss=0.012173174880445004\n",
      "Iteration 1805, loss=0.08799847960472107\n",
      "Iteration 1810, loss=0.011374901980161667\n",
      "Iteration 1815, loss=0.0616057887673378\n",
      "Iteration 1820, loss=0.010807649232447147\n",
      "Iteration 1825, loss=0.006980582140386105\n",
      "Iteration 1830, loss=0.07760606706142426\n",
      "Iteration 1835, loss=0.07899285107851028\n",
      "Iteration 1840, loss=0.009533197619020939\n",
      "Iteration 1845, loss=0.077062226831913\n",
      "Iteration 1850, loss=0.020592348650097847\n",
      "Iteration 1855, loss=0.024069631472229958\n",
      "Iteration 1860, loss=0.019482670351862907\n",
      "Iteration 1865, loss=0.0009024182800203562\n",
      "Iteration 1870, loss=0.07316125184297562\n",
      "Iteration 1875, loss=0.008450307883322239\n",
      "Iteration 1880, loss=0.037411920726299286\n",
      "Iteration 1885, loss=0.007151444908231497\n",
      "Iteration 1890, loss=0.06489034742116928\n",
      "Iteration 1895, loss=0.0007161881658248603\n",
      "Iteration 1900, loss=0.04461801052093506\n",
      "Iteration 1905, loss=0.004148089326918125\n",
      "Iteration 1910, loss=0.016109824180603027\n",
      "Iteration 1915, loss=0.06570099294185638\n",
      "Iteration 1920, loss=0.0037904856726527214\n",
      "Iteration 1925, loss=0.055061351507902145\n",
      "Iteration 1930, loss=0.0035301996394991875\n",
      "Iteration 1935, loss=0.0005225315690040588\n",
      "Iteration 1940, loss=0.006376962177455425\n",
      "Iteration 1945, loss=0.05691559985280037\n",
      "Iteration 1950, loss=0.013647034764289856\n",
      "Iteration 1955, loss=0.0585983544588089\n",
      "Iteration 1960, loss=0.0029061827808618546\n",
      "Iteration 1965, loss=0.004510450642555952\n",
      "Iteration 1970, loss=0.0515192486345768\n",
      "Iteration 1975, loss=0.04979512467980385\n",
      "Iteration 1980, loss=0.0003310322354082018\n",
      "Iteration 1985, loss=0.00238081649877131\n",
      "Iteration 1990, loss=0.011377297341823578\n",
      "Iteration 1995, loss=0.03185044974088669\n",
      "Iteration 2000, loss=0.0048388647846877575\n",
      "Iteration 2005, loss=0.0021178629249334335\n",
      "Iteration 2010, loss=0.000671340327244252\n",
      "Iteration 2015, loss=0.0020908580627292395\n",
      "Iteration 2020, loss=0.002026991220191121\n",
      "Iteration 2025, loss=0.04220329597592354\n",
      "Iteration 2030, loss=0.040514472872018814\n",
      "Iteration 2035, loss=0.02483418397605419\n",
      "Iteration 2040, loss=0.003898399183526635\n",
      "Iteration 2045, loss=0.03372679278254509\n",
      "Iteration 2050, loss=0.04493396729230881\n",
      "Iteration 2055, loss=0.043524064123630524\n",
      "Iteration 2060, loss=0.0059859175235033035\n",
      "Iteration 2065, loss=0.00228416221216321\n",
      "Iteration 2070, loss=0.005451412405818701\n",
      "Iteration 2075, loss=0.0012833129148930311\n",
      "Iteration 2080, loss=0.03884252905845642\n",
      "Iteration 2085, loss=0.035337526351213455\n",
      "Iteration 2090, loss=0.0077591752633452415\n",
      "Iteration 2095, loss=0.0032264557667076588\n",
      "Iteration 2100, loss=0.0031225807033479214\n",
      "Iteration 2105, loss=0.029574193060398102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2110, loss=0.0001319493749178946\n",
      "Iteration 2115, loss=0.028005508705973625\n",
      "Iteration 2120, loss=0.00011419018119340762\n",
      "Iteration 2125, loss=0.03132152929902077\n",
      "Iteration 2130, loss=0.002758654300123453\n",
      "Iteration 2135, loss=0.00010759483848232776\n",
      "Iteration 2140, loss=0.0038249718490988016\n",
      "Iteration 2145, loss=0.0013627787120640278\n",
      "Iteration 2150, loss=0.015629373490810394\n",
      "Iteration 2155, loss=0.0013008840614929795\n",
      "Iteration 2160, loss=0.02133755385875702\n",
      "Iteration 2165, loss=0.026615971699357033\n",
      "Iteration 2170, loss=0.025433529168367386\n",
      "Iteration 2175, loss=0.01397379394620657\n",
      "Iteration 2180, loss=0.013317315839231014\n",
      "Iteration 2185, loss=0.004995299503207207\n",
      "Iteration 2190, loss=0.0021243442315608263\n",
      "Iteration 2195, loss=0.001046987366862595\n",
      "Iteration 2200, loss=0.021418824791908264\n",
      "Iteration 2205, loss=0.000940675032325089\n",
      "Iteration 2210, loss=0.004109518602490425\n",
      "Iteration 2215, loss=0.016099821776151657\n",
      "Iteration 2220, loss=0.004322447348386049\n",
      "Iteration 2225, loss=0.00046247176942415535\n",
      "Iteration 2230, loss=0.003485842840746045\n",
      "Iteration 2235, loss=0.000431950727943331\n",
      "Iteration 2240, loss=0.0005383741809055209\n",
      "Iteration 2245, loss=9.942940232576802e-05\n",
      "Iteration 2250, loss=0.01653318665921688\n",
      "Iteration 2255, loss=0.009373735636472702\n",
      "Iteration 2260, loss=0.000649125431664288\n",
      "Iteration 2265, loss=0.0006104620988480747\n",
      "Iteration 2270, loss=0.0015018499689176679\n",
      "Iteration 2275, loss=2.947792199847754e-05\n",
      "Iteration 2280, loss=0.0016534763853996992\n",
      "Iteration 2285, loss=0.018571721389889717\n",
      "Iteration 2290, loss=0.013856248930096626\n",
      "Iteration 2295, loss=0.0013191546313464642\n",
      "Iteration 2300, loss=0.00042240277980454266\n",
      "Iteration 2305, loss=0.00047994652413763106\n",
      "Iteration 2310, loss=0.0022479016333818436\n",
      "Iteration 2315, loss=6.09587732469663e-05\n",
      "Iteration 2320, loss=0.001227157306857407\n",
      "Iteration 2325, loss=0.0012448277557268739\n",
      "Iteration 2330, loss=0.001197406672872603\n",
      "Iteration 2335, loss=0.009627196937799454\n",
      "Iteration 2340, loss=0.0011508436873555183\n",
      "Iteration 2345, loss=0.014755240641534328\n",
      "Iteration 2350, loss=0.0016094899037852883\n",
      "Iteration 2355, loss=0.0015268299030140042\n",
      "Iteration 2360, loss=0.014013206586241722\n",
      "Iteration 2365, loss=0.005943582858890295\n",
      "Iteration 2370, loss=0.0008732779533602297\n",
      "Iteration 2375, loss=0.0002625361958052963\n",
      "Iteration 2380, loss=0.002124728634953499\n",
      "Iteration 2385, loss=0.0020073927007615566\n",
      "Iteration 2390, loss=0.004810294136404991\n",
      "Iteration 2395, loss=0.0019016298465430737\n",
      "Iteration 2400, loss=0.009126237593591213\n",
      "Iteration 2405, loss=0.0006609762203879654\n",
      "Iteration 2410, loss=0.0008743929793126881\n",
      "Iteration 2415, loss=0.0017368259141221642\n",
      "Iteration 2420, loss=0.004293710924685001\n",
      "Iteration 2425, loss=0.0001729657087707892\n",
      "Iteration 2430, loss=0.0001562288380227983\n",
      "Iteration 2435, loss=0.00015954177069943398\n",
      "Iteration 2440, loss=0.00015702538075856864\n",
      "Iteration 2445, loss=0.007474069949239492\n",
      "Iteration 2450, loss=0.00010085642134072259\n",
      "Iteration 2455, loss=0.005673112813383341\n",
      "Iteration 2460, loss=0.00013589012087322772\n",
      "Iteration 2465, loss=0.009277455508708954\n",
      "Iteration 2470, loss=8.304687071358785e-05\n",
      "Iteration 2475, loss=0.0001179450482595712\n",
      "Iteration 2480, loss=7.552631723228842e-05\n",
      "Iteration 2485, loss=0.004725159611552954\n",
      "Iteration 2490, loss=0.008371244184672832\n",
      "Iteration 2495, loss=0.0029933901969343424\n",
      "Iteration 2500, loss=6.662916712230071e-05\n",
      "Iteration 2505, loss=0.006156439892947674\n",
      "Iteration 2510, loss=1.2470171895984095e-05\n",
      "Iteration 2515, loss=0.0003198490012437105\n",
      "Iteration 2520, loss=0.003918818198144436\n",
      "Iteration 2525, loss=0.001064227893948555\n",
      "Iteration 2530, loss=0.0035791874397546053\n",
      "Iteration 2535, loss=1.0277817636961117e-05\n",
      "Iteration 2540, loss=9.670683539297897e-06\n",
      "Iteration 2545, loss=0.0004902069922536612\n",
      "Iteration 2550, loss=0.0032804689835757017\n",
      "Iteration 2555, loss=0.0022145037073642015\n",
      "Iteration 2560, loss=0.002124007558450103\n",
      "Iteration 2565, loss=0.0004379373276606202\n",
      "Iteration 2570, loss=6.438678974518552e-05\n",
      "Iteration 2575, loss=6.356844824040309e-05\n",
      "Iteration 2580, loss=0.005797584541141987\n",
      "Iteration 2585, loss=3.542316335369833e-05\n",
      "Iteration 2590, loss=0.0018613791326060891\n",
      "Iteration 2595, loss=3.302108234493062e-05\n",
      "Iteration 2600, loss=0.00543575594201684\n",
      "Iteration 2605, loss=0.0020807278342545033\n",
      "Iteration 2610, loss=4.854612052440643e-05\n",
      "Iteration 2615, loss=5.53123936697375e-06\n",
      "Iteration 2620, loss=4.6898356231395155e-05\n",
      "Iteration 2625, loss=0.0020929984748363495\n",
      "Iteration 2630, loss=2.497806963219773e-05\n",
      "Iteration 2635, loss=1.5474940937565407e-06\n",
      "Iteration 2640, loss=2.3296865037991665e-05\n",
      "Iteration 2645, loss=3.5774493881035596e-05\n",
      "Iteration 2650, loss=0.00013738608686253428\n",
      "Iteration 2655, loss=0.00015381307457573712\n",
      "Iteration 2660, loss=0.004619905725121498\n",
      "Iteration 2665, loss=0.0017477639485150576\n",
      "Iteration 2670, loss=0.00011464540148153901\n",
      "Iteration 2675, loss=3.0566818168153986e-05\n",
      "Iteration 2680, loss=3.3640678793744883e-06\n",
      "Iteration 2685, loss=0.0005053976201452315\n",
      "Iteration 2690, loss=0.00048750737914815545\n",
      "Iteration 2695, loss=2.4638384275021963e-05\n",
      "Iteration 2700, loss=9.636059257900342e-05\n",
      "Iteration 2705, loss=1.3162959476176184e-05\n",
      "Iteration 2710, loss=0.00011264228669460863\n",
      "Iteration 2715, loss=0.0009930089581757784\n",
      "Iteration 2720, loss=0.0011204045731574297\n",
      "Iteration 2725, loss=0.00039803615072742105\n",
      "Iteration 2730, loss=0.0025504897348582745\n",
      "Iteration 2735, loss=0.00035462406231090426\n",
      "Iteration 2740, loss=0.00019073676958214492\n",
      "Iteration 2745, loss=1.0411639777885284e-05\n",
      "Iteration 2750, loss=9.96548988041468e-06\n",
      "Iteration 2755, loss=9.584039617038798e-06\n",
      "Iteration 2760, loss=4.7269753622458666e-07\n",
      "Iteration 2765, loss=0.00016763225721661001\n",
      "Iteration 2770, loss=4.2645737607927003e-07\n",
      "Iteration 2775, loss=0.0029497018549591303\n",
      "Iteration 2780, loss=1.166888637271768e-06\n",
      "Iteration 2785, loss=0.000773758627474308\n",
      "Iteration 2790, loss=0.0007361963507719338\n",
      "Iteration 2795, loss=1.1448430086602457e-05\n",
      "Iteration 2800, loss=0.0006404932937584817\n",
      "Iteration 2805, loss=8.322344342559518e-07\n",
      "Iteration 2810, loss=4.8011545004555956e-05\n",
      "Iteration 2815, loss=7.855570629544673e-07\n",
      "Iteration 2820, loss=0.0005724395159631968\n",
      "Iteration 2825, loss=7.448819019373332e-07\n",
      "Iteration 2830, loss=0.0021478254348039627\n",
      "Iteration 2835, loss=4.875787635683082e-05\n",
      "Iteration 2840, loss=3.626484249252826e-05\n",
      "Iteration 2845, loss=3.961213224101812e-06\n",
      "Iteration 2850, loss=0.0002356634067837149\n",
      "Iteration 2855, loss=0.0005844752886332572\n",
      "Iteration 2860, loss=4.036666723550297e-05\n",
      "Iteration 2865, loss=0.00010428954556118697\n",
      "Iteration 2870, loss=0.0002194675907958299\n",
      "Iteration 2875, loss=1.6869830687937792e-07\n",
      "Iteration 2880, loss=1.5053541346787824e-07\n",
      "Iteration 2885, loss=1.3699752798856935e-07\n",
      "Iteration 2890, loss=3.194805685780011e-05\n",
      "Iteration 2895, loss=2.2771895601181313e-05\n",
      "Iteration 2900, loss=4.2035856040456565e-07\n",
      "Iteration 2905, loss=8.372597221750766e-05\n",
      "Iteration 2910, loss=8.048767631407827e-05\n",
      "Iteration 2915, loss=2.277748535561841e-06\n",
      "Iteration 2920, loss=2.2835769414086826e-05\n",
      "Iteration 2925, loss=0.00032882412779144943\n",
      "Iteration 2930, loss=0.0008935182704590261\n",
      "Iteration 2935, loss=1.7541478882776573e-05\n",
      "Iteration 2940, loss=0.00027926030452363193\n",
      "Iteration 2945, loss=0.0002640981983859092\n",
      "Iteration 2950, loss=0.0011089203180745244\n",
      "Iteration 2955, loss=0.0007694991654716432\n",
      "Iteration 2960, loss=1.609467653906904e-05\n",
      "Iteration 2965, loss=3.3752896797523135e-06\n",
      "Iteration 2970, loss=0.00028515816666185856\n",
      "Iteration 2975, loss=0.000117457915621344\n",
      "Iteration 2980, loss=6.123559433035553e-05\n",
      "Iteration 2985, loss=0.0002538651169743389\n",
      "Iteration 2990, loss=9.715414307720494e-06\n",
      "Iteration 2995, loss=5.9127259532942844e-08\n",
      "Iteration 3000, loss=8.296198757307138e-06\n",
      "Iteration 3005, loss=0.00020318552560638636\n",
      "Iteration 3010, loss=0.00018131191609427333\n",
      "Iteration 3015, loss=1.6802055142761674e-06\n",
      "Iteration 3020, loss=1.2449999076125096e-07\n",
      "Iteration 3025, loss=6.960858058846497e-07\n",
      "Iteration 3030, loss=8.381208317587152e-06\n",
      "Iteration 3035, loss=1.3646786101162434e-06\n",
      "Iteration 3040, loss=7.710346835665405e-06\n",
      "Iteration 3045, loss=1.8117295894626295e-06\n",
      "Iteration 3050, loss=0.00013793769176118076\n",
      "Iteration 3055, loss=0.00047076784539967775\n",
      "Iteration 3060, loss=0.00012628721015062183\n",
      "Iteration 3065, loss=2.4787599528508508e-08\n",
      "Iteration 3070, loss=4.785598775924882e-06\n",
      "Iteration 3075, loss=4.3803808580378245e-07\n",
      "Iteration 3080, loss=0.00010644792200764641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3085, loss=6.642634531317526e-08\n",
      "Iteration 3090, loss=3.681460293591954e-05\n",
      "Iteration 3095, loss=9.280561789637432e-05\n",
      "Iteration 3100, loss=6.886130722705275e-05\n",
      "Iteration 3105, loss=3.4331293136347085e-05\n",
      "Iteration 3110, loss=7.305317808459222e-07\n",
      "Iteration 3115, loss=4.6858609437094856e-08\n",
      "Iteration 3120, loss=8.087938476819545e-05\n",
      "Iteration 3125, loss=9.355732686344709e-07\n",
      "Iteration 3130, loss=3.0590610549552366e-05\n",
      "Iteration 3135, loss=2.4790406314423308e-06\n",
      "Iteration 3140, loss=4.055390334656295e-08\n",
      "Iteration 3145, loss=0.0002849665470421314\n",
      "Iteration 3150, loss=7.438752618327271e-07\n",
      "Iteration 3155, loss=5.686108852387406e-05\n",
      "Iteration 3160, loss=2.5849269150057808e-05\n",
      "Iteration 3165, loss=2.3673122996115126e-05\n",
      "Iteration 3170, loss=1.639107978235188e-07\n",
      "Iteration 3175, loss=1.5689045085309772e-07\n",
      "Iteration 3180, loss=1.4476476906111202e-07\n",
      "Iteration 3185, loss=0.00011187503696419299\n",
      "Iteration 3190, loss=4.0832989611772064e-07\n",
      "Iteration 3195, loss=9.792298078536987e-05\n",
      "Iteration 3200, loss=1.286981330395065e-07\n",
      "Iteration 3205, loss=1.234858757470647e-07\n",
      "Iteration 3210, loss=1.432103317711153e-06\n",
      "Iteration 3215, loss=4.500254362937994e-05\n",
      "Iteration 3220, loss=3.240658372760663e-07\n",
      "Iteration 3225, loss=1.1228366929572076e-06\n",
      "Iteration 3230, loss=1.648948818910867e-05\n",
      "Iteration 3235, loss=3.8356491131708026e-05\n",
      "Iteration 3240, loss=8.186647448837903e-08\n",
      "Iteration 3245, loss=3.117961387033574e-05\n",
      "Iteration 3250, loss=4.721241619876082e-09\n",
      "Iteration 3255, loss=6.700152152916417e-05\n",
      "Iteration 3260, loss=1.2475444464143948e-06\n",
      "Iteration 3265, loss=6.306150316959247e-05\n",
      "Iteration 3270, loss=0.000219205510802567\n",
      "Iteration 3275, loss=1.1362348004695377e-06\n",
      "Iteration 3280, loss=1.1204230432326767e-08\n",
      "Iteration 3285, loss=6.343750555970473e-07\n",
      "Iteration 3290, loss=1.2617560969374608e-05\n",
      "Iteration 3295, loss=2.630081507959403e-05\n",
      "Iteration 3300, loss=4.390571817225464e-08\n",
      "Iteration 3305, loss=1.1155513675475959e-05\n",
      "Iteration 3310, loss=2.7439610583712692e-09\n",
      "Iteration 3315, loss=4.6957845256656583e-07\n",
      "Iteration 3320, loss=3.664655778834458e-08\n",
      "Iteration 3325, loss=4.662147694034502e-05\n",
      "Iteration 3330, loss=7.136705004739952e-09\n",
      "Iteration 3335, loss=3.873854268476862e-07\n",
      "Iteration 3340, loss=6.68366828548983e-09\n",
      "Iteration 3345, loss=0.00010701382416300476\n",
      "Iteration 3350, loss=6.361017934608526e-09\n",
      "Iteration 3355, loss=8.964575926029283e-08\n",
      "Iteration 3360, loss=1.3556258693370182e-07\n",
      "Iteration 3365, loss=0.00012170620175311342\n",
      "Iteration 3370, loss=5.52642154261207e-09\n",
      "Iteration 3375, loss=1.2479573285872902e-07\n",
      "Iteration 3380, loss=0.00011270889808656648\n",
      "Iteration 3385, loss=1.2455896467145067e-05\n",
      "Iteration 3390, loss=2.3439380925083242e-07\n",
      "Iteration 3395, loss=1.4975593330746051e-05\n",
      "Iteration 3400, loss=0.00010319035209249705\n",
      "Iteration 3405, loss=1.8716843896982027e-07\n",
      "Iteration 3410, loss=7.816951983841136e-05\n",
      "Iteration 3415, loss=1.214482108480297e-05\n",
      "Iteration 3420, loss=1.1341531717334874e-05\n",
      "Iteration 3425, loss=2.5676965378806926e-05\n",
      "Iteration 3430, loss=1.0134642252523918e-05\n",
      "Iteration 3435, loss=2.690056533083407e-07\n",
      "Iteration 3440, loss=2.282695277244784e-05\n",
      "Iteration 3445, loss=5.600748863798799e-06\n",
      "Iteration 3450, loss=1.926293953147251e-05\n",
      "Iteration 3455, loss=2.2709651936736464e-09\n",
      "Iteration 3460, loss=4.887629074801225e-06\n",
      "Iteration 3465, loss=5.727371171815321e-05\n",
      "Iteration 3470, loss=1.1257896659344624e-07\n",
      "Iteration 3475, loss=1.821117479039458e-07\n",
      "Iteration 3480, loss=8.33142355105565e-09\n",
      "Iteration 3485, loss=4.752040695166215e-05\n",
      "Iteration 3490, loss=1.4780073831488494e-09\n",
      "Iteration 3495, loss=4.375952630653046e-05\n",
      "Iteration 3500, loss=6.793933859938761e-09\n",
      "Iteration 3505, loss=5.918457191000925e-06\n",
      "Iteration 3510, loss=3.7600133509840816e-05\n",
      "Iteration 3515, loss=1.1526265497607824e-09\n",
      "Iteration 3520, loss=2.824097045106555e-08\n",
      "Iteration 3525, loss=5.801228326163255e-06\n",
      "Iteration 3530, loss=5.226664598012576e-06\n",
      "Iteration 3535, loss=1.808615124332391e-08\n",
      "Iteration 3540, loss=4.839673692913493e-06\n",
      "Iteration 3545, loss=7.20880564131221e-08\n",
      "Iteration 3550, loss=4.4145563151687384e-05\n",
      "Iteration 3555, loss=2.310221316292882e-05\n",
      "Iteration 3560, loss=7.267777490937988e-10\n",
      "Iteration 3565, loss=4.2340243453509174e-06\n",
      "Iteration 3570, loss=3.9710625969746616e-06\n",
      "Iteration 3575, loss=2.2824017378297867e-06\n",
      "Iteration 3580, loss=9.79767031394907e-11\n",
      "Iteration 3585, loss=2.1724504222220276e-06\n",
      "Iteration 3590, loss=3.3477681427029893e-06\n",
      "Iteration 3595, loss=3.33858588419389e-06\n",
      "Iteration 3600, loss=2.470327672199346e-06\n",
      "Iteration 3605, loss=3.170459194734576e-06\n",
      "Iteration 3610, loss=3.877286547382397e-10\n",
      "Iteration 3615, loss=2.3449699710909044e-06\n",
      "Iteration 3620, loss=3.1935294828144833e-05\n",
      "Iteration 3625, loss=9.417952639978466e-09\n",
      "Iteration 3630, loss=9.2827354691849e-09\n",
      "Iteration 3635, loss=2.768308604572667e-06\n",
      "Iteration 3640, loss=2.0075245910078365e-09\n",
      "Iteration 3645, loss=4.8775362415653944e-08\n",
      "Iteration 3650, loss=4.76645875546744e-11\n",
      "Iteration 3655, loss=1.4291822481027339e-06\n",
      "Iteration 3660, loss=1.331589260189503e-06\n",
      "Iteration 3665, loss=4.00669932787423e-08\n",
      "Iteration 3670, loss=2.1290195491019404e-06\n",
      "Iteration 3675, loss=3.640418810846313e-08\n",
      "Iteration 3680, loss=3.4303290874504455e-08\n",
      "Iteration 3685, loss=1.6892315670702374e-06\n",
      "Iteration 3690, loss=1.7758951798896305e-05\n",
      "Iteration 3695, loss=1.2487463209254202e-05\n",
      "Iteration 3700, loss=2.949625077519613e-08\n",
      "Iteration 3705, loss=4.864582514585436e-09\n",
      "Iteration 3710, loss=3.3193027775269e-06\n",
      "Iteration 3715, loss=2.5780533263741745e-08\n",
      "Iteration 3720, loss=1.2087050249576237e-09\n",
      "Iteration 3725, loss=1.4326942410214372e-10\n",
      "Iteration 3730, loss=1.5598350273648975e-06\n",
      "Iteration 3735, loss=8.529511433152948e-06\n",
      "Iteration 3740, loss=3.802672399189078e-09\n",
      "Iteration 3745, loss=1.0675822448291683e-09\n",
      "Iteration 3750, loss=3.624336608609724e-09\n",
      "Iteration 3755, loss=1.3207103813361876e-11\n",
      "Iteration 3760, loss=1.2782532394339796e-05\n",
      "Iteration 3765, loss=2.272755637022783e-06\n",
      "Iteration 3770, loss=7.655111176063656e-07\n",
      "Iteration 3775, loss=6.84049496157968e-07\n",
      "Iteration 3780, loss=2.1486500827450072e-06\n",
      "Iteration 3785, loss=1.0803303212014725e-06\n",
      "Iteration 3790, loss=1.0368388757342473e-05\n",
      "Iteration 3795, loss=9.432645811102702e-07\n",
      "Iteration 3800, loss=6.123687668058153e-11\n",
      "Iteration 3805, loss=5.863348001566848e-11\n",
      "Iteration 3810, loss=7.851114105505985e-07\n",
      "Iteration 3815, loss=6.914108894306992e-07\n",
      "Iteration 3820, loss=2.8581592648180276e-09\n",
      "Iteration 3825, loss=8.036584517867595e-07\n",
      "Iteration 3830, loss=6.013100346535794e-07\n",
      "Iteration 3835, loss=4.818910497306206e-07\n",
      "Iteration 3840, loss=4.990981778973946e-07\n",
      "Iteration 3845, loss=2.2400136856504105e-08\n",
      "Iteration 3850, loss=5.002989134084146e-10\n",
      "Iteration 3855, loss=4.619430740149255e-07\n",
      "Iteration 3860, loss=1.1823241052866251e-08\n",
      "Iteration 3865, loss=5.917785983911017e-06\n",
      "Iteration 3870, loss=1.216260329783836e-06\n",
      "Iteration 3875, loss=4.4929551745731544e-10\n",
      "Iteration 3880, loss=9.720989346817532e-09\n",
      "Iteration 3885, loss=3.928510636797e-07\n",
      "Iteration 3890, loss=3.817211563728051e-06\n",
      "Iteration 3895, loss=3.2787238524178974e-06\n",
      "Iteration 3900, loss=3.8326411488931456e-10\n",
      "Iteration 3905, loss=4.616541957602749e-07\n",
      "Iteration 3910, loss=3.0442447496170644e-06\n",
      "Iteration 3915, loss=8.537091957805387e-07\n",
      "Iteration 3920, loss=6.244207373384825e-10\n",
      "Iteration 3925, loss=3.235507506360591e-07\n",
      "Iteration 3930, loss=2.3684858328071323e-12\n",
      "Iteration 3935, loss=2.250459658625914e-07\n",
      "Iteration 3940, loss=4.013621037302073e-06\n",
      "Iteration 3945, loss=1.5359409299975368e-09\n",
      "Iteration 3950, loss=4.988905399905263e-10\n",
      "Iteration 3955, loss=1.7369147897738912e-08\n",
      "Iteration 3960, loss=2.950770183751672e-10\n",
      "Iteration 3965, loss=3.2543721317779273e-07\n",
      "Iteration 3970, loss=1.6706612981920443e-08\n",
      "Iteration 3975, loss=1.8021268033407978e-07\n",
      "Iteration 3980, loss=2.3812313543203345e-07\n",
      "Iteration 3985, loss=1.253907351214667e-12\n",
      "Iteration 3990, loss=2.023583419941133e-06\n",
      "Iteration 3995, loss=1.1349277073691155e-09\n",
      "Iteration 4000, loss=1.9874238432748825e-07\n",
      "Iteration 4005, loss=5.278409709319476e-09\n",
      "Iteration 4010, loss=1.879992339581804e-07\n",
      "Iteration 4015, loss=2.8526622730673523e-10\n",
      "Iteration 4020, loss=4.913538909079307e-09\n",
      "Iteration 4025, loss=1.363065678106068e-07\n",
      "Iteration 4030, loss=1.0176822717866685e-09\n",
      "Iteration 4035, loss=1.4285345351083834e-08\n",
      "Iteration 4040, loss=1.4276901438847744e-08\n",
      "Iteration 4045, loss=1.0923083948455314e-07\n",
      "Iteration 4050, loss=2.2724111481409182e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4055, loss=1.1910933039871452e-07\n",
      "Iteration 4060, loss=1.6301140992602825e-10\n",
      "Iteration 4065, loss=5.056023308358526e-12\n",
      "Iteration 4070, loss=1.5686972554274092e-10\n",
      "Iteration 4075, loss=4.97390757053795e-12\n",
      "Iteration 4080, loss=1.341676814092807e-08\n",
      "Iteration 4085, loss=7.75727713175911e-08\n",
      "Iteration 4090, loss=1.3041614010944613e-07\n",
      "Iteration 4095, loss=1.6266425006961072e-07\n",
      "Iteration 4100, loss=1.2903715429501972e-08\n",
      "Iteration 4105, loss=3.797187390908174e-12\n",
      "Iteration 4110, loss=1.1668519306340386e-07\n",
      "Iteration 4115, loss=3.595673455161763e-12\n",
      "Iteration 4120, loss=7.811694047177298e-08\n",
      "Iteration 4125, loss=1.1788012233004963e-10\n",
      "Iteration 4130, loss=3.284104758971562e-12\n",
      "Iteration 4135, loss=9.307932913316108e-08\n",
      "Iteration 4140, loss=2.7662363510927435e-09\n",
      "Iteration 4145, loss=1.1783034103984846e-08\n",
      "Iteration 4150, loss=5.467845909379321e-08\n",
      "Iteration 4155, loss=6.735141511171605e-08\n",
      "Iteration 4160, loss=1.8991731565165537e-07\n",
      "Iteration 4165, loss=2.4495625527976017e-09\n",
      "Iteration 4170, loss=9.902736337608076e-07\n",
      "Iteration 4175, loss=7.325807160896147e-08\n",
      "Iteration 4180, loss=2.2749666594990003e-09\n",
      "Iteration 4185, loss=4.7563585070520276e-08\n",
      "Iteration 4190, loss=6.608589586676317e-08\n",
      "Iteration 4195, loss=5.988853163385599e-11\n",
      "Iteration 4200, loss=6.02388396941933e-08\n",
      "Iteration 4205, loss=1.4834947235486257e-13\n",
      "Iteration 4210, loss=8.591981986993957e-11\n",
      "Iteration 4215, loss=1.2767857517775871e-13\n",
      "Iteration 4220, loss=4.627564720305344e-10\n",
      "Iteration 4225, loss=4.671805342582047e-08\n",
      "Iteration 4230, loss=7.138123692129739e-08\n",
      "Iteration 4235, loss=6.436641086793315e-08\n",
      "Iteration 4240, loss=7.500714787056495e-07\n",
      "Iteration 4245, loss=1.4249612774464016e-12\n",
      "Iteration 4250, loss=1.896836465675733e-09\n",
      "Iteration 4255, loss=3.959774175221753e-10\n",
      "Iteration 4260, loss=4.208664319826916e-11\n",
      "Iteration 4265, loss=8.809606825499819e-14\n",
      "Iteration 4270, loss=3.9529126499848743e-11\n",
      "Iteration 4275, loss=1.2294201030479956e-12\n",
      "Iteration 4280, loss=3.579333807124385e-08\n",
      "Iteration 4285, loss=1.1495883455239775e-12\n",
      "Iteration 4290, loss=1.1314742545676104e-12\n",
      "Iteration 4295, loss=1.035576318031417e-07\n",
      "Iteration 4300, loss=3.3550545452953884e-08\n",
      "Iteration 4305, loss=8.746876112297741e-09\n",
      "Iteration 4310, loss=1.562069806837485e-09\n",
      "Iteration 4315, loss=1.0513040091253423e-12\n",
      "Iteration 4320, loss=2.8272552299313247e-08\n",
      "Iteration 4325, loss=8.581590549283646e-09\n",
      "Iteration 4330, loss=3.423723804463208e-10\n",
      "Iteration 4335, loss=3.476918095657311e-07\n",
      "Iteration 4340, loss=2.67220152255776e-11\n",
      "Iteration 4345, loss=9.598137172203702e-13\n",
      "Iteration 4350, loss=4.1935102501611254e-08\n",
      "Iteration 4355, loss=2.5450843210705898e-08\n",
      "Iteration 4360, loss=2.8422277864592616e-07\n",
      "Iteration 4365, loss=5.314550188817435e-11\n",
      "Iteration 4370, loss=3.76005644397992e-08\n",
      "Iteration 4375, loss=7.821141195622772e-13\n",
      "Iteration 4380, loss=2.2777465247258988e-08\n",
      "Iteration 4385, loss=2.2817283706899616e-07\n",
      "Iteration 4390, loss=7.689275882682978e-09\n",
      "Iteration 4395, loss=2.147849187394968e-08\n",
      "Iteration 4400, loss=2.1619031976942427e-14\n",
      "Iteration 4405, loss=2.1324609633666114e-07\n",
      "Iteration 4410, loss=7.440969174155043e-09\n",
      "Iteration 4415, loss=1.1129698274103816e-09\n",
      "Iteration 4420, loss=2.827488287948654e-07\n",
      "Iteration 4425, loss=1.6185755860598228e-11\n",
      "Iteration 4430, loss=1.5727320487601837e-11\n",
      "Iteration 4435, loss=6.164274277652149e-13\n",
      "Iteration 4440, loss=1.7749092862118232e-08\n",
      "Iteration 4445, loss=1.7688124742676337e-08\n",
      "Iteration 4450, loss=1.832931673106941e-08\n",
      "Iteration 4455, loss=2.4824242661480866e-10\n",
      "Iteration 4460, loss=1.7103582550248575e-08\n",
      "Iteration 4465, loss=1.449173936407533e-07\n",
      "Iteration 4470, loss=1.6254276147265045e-08\n",
      "Iteration 4475, loss=1.3410046051376412e-07\n",
      "Iteration 4480, loss=1.7369611526873996e-08\n",
      "Iteration 4485, loss=2.6580991629998607e-08\n",
      "Iteration 4490, loss=6.770364269215179e-09\n",
      "Iteration 4495, loss=6.760016102447253e-09\n",
      "Iteration 4500, loss=2.5875806386466138e-08\n",
      "Iteration 4505, loss=1.2270228921806847e-07\n",
      "Iteration 4510, loss=6.503660721079996e-09\n",
      "Iteration 4515, loss=1.69980083342125e-07\n",
      "Iteration 4520, loss=2.3175157792820755e-08\n",
      "Iteration 4525, loss=3.8886366127144356e-08\n",
      "Iteration 4530, loss=6.338575886388753e-09\n",
      "Iteration 4535, loss=3.566746542560395e-08\n",
      "Iteration 4540, loss=1.4319553542918584e-08\n",
      "Iteration 4545, loss=3.4471462129204156e-08\n",
      "Iteration 4550, loss=1.3254972230924977e-08\n",
      "Iteration 4555, loss=2.0782695009380348e-10\n",
      "Iteration 4560, loss=1.2810382088446204e-08\n",
      "Iteration 4565, loss=6.277184989248591e-15\n",
      "Iteration 4570, loss=3.101257098347965e-11\n",
      "Iteration 4575, loss=5.9661484641537754e-09\n",
      "Iteration 4580, loss=1.840014363096998e-08\n",
      "Iteration 4585, loss=1.1850084469244848e-08\n",
      "Iteration 4590, loss=1.1353266771152448e-08\n",
      "Iteration 4595, loss=2.720418024182436e-08\n",
      "Iteration 4600, loss=6.277374019897364e-12\n",
      "Iteration 4605, loss=5.826901850971731e-15\n",
      "Iteration 4610, loss=1.056772447327603e-08\n",
      "Iteration 4615, loss=1.055104803526774e-08\n",
      "Iteration 4620, loss=1.0338586875491274e-08\n",
      "Iteration 4625, loss=5.579901558167011e-12\n",
      "Iteration 4630, loss=1.6226644916628175e-08\n",
      "Iteration 4635, loss=9.169099257633206e-08\n",
      "Iteration 4640, loss=5.336182884452256e-12\n",
      "Iteration 4645, loss=9.951155455212302e-09\n",
      "Iteration 4650, loss=2.6040242878666575e-11\n",
      "Iteration 4655, loss=9.905272158050593e-09\n",
      "Iteration 4660, loss=3.2466284977279714e-13\n",
      "Iteration 4665, loss=7.912448296565344e-08\n",
      "Iteration 4670, loss=3.715294876762386e-15\n",
      "Iteration 4675, loss=3.7020667632251154e-15\n",
      "Iteration 4680, loss=5.553840165539725e-10\n",
      "Iteration 4685, loss=3.0544604389184937e-13\n",
      "Iteration 4690, loss=5.392852275853954e-10\n",
      "Iteration 4695, loss=4.876289949606871e-08\n",
      "Iteration 4700, loss=5.0936548312563446e-09\n",
      "Iteration 4705, loss=1.930530046934109e-08\n",
      "Iteration 4710, loss=8.439316800945562e-09\n",
      "Iteration 4715, loss=3.887716537587238e-12\n",
      "Iteration 4720, loss=4.940123421448561e-09\n",
      "Iteration 4725, loss=1.756206025049778e-08\n",
      "Iteration 4730, loss=4.791463892317438e-10\n",
      "Iteration 4735, loss=8.017893016187827e-09\n",
      "Iteration 4740, loss=1.6092293719793815e-08\n",
      "Iteration 4745, loss=4.5559683803375606e-10\n",
      "Iteration 4750, loss=3.578617224775371e-08\n",
      "Iteration 4755, loss=1.1749617279122049e-08\n",
      "Iteration 4760, loss=6.883885017572311e-09\n",
      "Iteration 4765, loss=4.767291450491484e-09\n",
      "Iteration 4770, loss=2.5974442865658864e-13\n",
      "Iteration 4775, loss=3.1443003582765083e-12\n",
      "Iteration 4780, loss=7.539113333621117e-09\n",
      "Iteration 4785, loss=1.1159186463771675e-08\n",
      "Iteration 4790, loss=6.462027357656552e-09\n",
      "Iteration 4795, loss=3.019803784809483e-08\n",
      "Iteration 4800, loss=4.5721537667020584e-09\n",
      "Iteration 4805, loss=4.1463504851790844e-10\n",
      "Iteration 4810, loss=1.562389752418473e-15\n",
      "Iteration 4815, loss=6.929874452055174e-09\n",
      "Iteration 4820, loss=2.6955897070024548e-08\n",
      "Iteration 4825, loss=6.787910677985565e-09\n",
      "Iteration 4830, loss=4.446444101802172e-09\n",
      "Iteration 4835, loss=1.429822510390366e-10\n",
      "Iteration 4840, loss=1.4245198076690002e-10\n",
      "Iteration 4845, loss=5.546559656011141e-09\n",
      "Iteration 4850, loss=3.858827701819223e-10\n",
      "Iteration 4855, loss=2.1869334956240577e-13\n",
      "Iteration 4860, loss=1.3976067525511837e-10\n",
      "Iteration 4865, loss=1.3945869459242033e-10\n",
      "Iteration 4870, loss=1.3945869459242033e-10\n",
      "Iteration 4875, loss=6.111418482390718e-09\n",
      "Iteration 4880, loss=3.682091020973388e-10\n",
      "Iteration 4885, loss=1.2304151998955288e-15\n",
      "Iteration 4890, loss=6.263271679074478e-09\n",
      "Iteration 4895, loss=5.906895417240321e-09\n",
      "Iteration 4900, loss=2.0673438072549288e-08\n",
      "Iteration 4905, loss=1.9912217220138112e-13\n",
      "Iteration 4910, loss=3.562072581342335e-10\n",
      "Iteration 4915, loss=1.6581952824723523e-11\n",
      "Iteration 4920, loss=5.7188946911423955e-09\n",
      "Iteration 4925, loss=1.0191026689199134e-08\n",
      "Iteration 4930, loss=1.0295103138724783e-15\n",
      "Iteration 4935, loss=1.027399613647039e-15\n",
      "Iteration 4940, loss=9.986199751981797e-16\n",
      "Iteration 4945, loss=5.507438505247819e-09\n",
      "Iteration 4950, loss=4.018905208624801e-09\n",
      "Iteration 4955, loss=5.413817838473278e-09\n",
      "Iteration 4960, loss=4.4246930563929254e-09\n",
      "Iteration 4965, loss=8.089938496880222e-09\n",
      "Iteration 4970, loss=3.272047077729212e-10\n",
      "Iteration 4975, loss=4.368425177148083e-09\n",
      "Iteration 4980, loss=1.693642947241114e-12\n",
      "Iteration 4985, loss=5.3210880146536965e-09\n",
      "Iteration 4990, loss=3.189460084929152e-10\n",
      "Iteration 4995, loss=4.2469801009303865e-09\n",
      "Iteration 5000, loss=5.0006065954733e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5005, loss=8.462804643597833e-16\n",
      "Iteration 5010, loss=1.625064882659899e-08\n",
      "Iteration 5015, loss=4.20245216403714e-09\n",
      "Iteration 5020, loss=3.11335485170261e-10\n",
      "Iteration 5025, loss=3.888849686717322e-09\n",
      "Iteration 5030, loss=7.833296639341133e-16\n",
      "Iteration 5035, loss=2.786508801477794e-08\n",
      "Iteration 5040, loss=4.8400838892348474e-09\n",
      "Iteration 5045, loss=3.858799946243607e-09\n",
      "Iteration 5050, loss=1.2207126975916083e-10\n",
      "Iteration 5055, loss=4.744757475805272e-09\n",
      "Iteration 5060, loss=7.764277218425301e-16\n",
      "Iteration 5065, loss=3.846858387390739e-09\n",
      "Iteration 5070, loss=3.0664432104643424e-10\n",
      "Iteration 5075, loss=4.875041259566615e-09\n",
      "Iteration 5080, loss=2.7350688824867575e-08\n",
      "Iteration 5085, loss=4.054228508465485e-09\n",
      "Iteration 5090, loss=1.2002067395489036e-10\n",
      "Iteration 5095, loss=4.666112829454505e-09\n",
      "Iteration 5100, loss=2.9682745150694245e-10\n",
      "Iteration 5105, loss=1.4557515349428174e-12\n",
      "Iteration 5110, loss=3.776405854694076e-09\n",
      "Iteration 5115, loss=1.1836090441086355e-10\n",
      "Iteration 5120, loss=2.9319480177036894e-10\n",
      "Iteration 5125, loss=4.297787903340122e-09\n",
      "Iteration 5130, loss=1.427014756361089e-12\n",
      "Iteration 5135, loss=3.917893121041516e-09\n",
      "Iteration 5140, loss=3.74591735408103e-09\n",
      "Iteration 5145, loss=4.248454477107089e-09\n",
      "Iteration 5150, loss=2.6921160412030076e-08\n",
      "Iteration 5155, loss=7.326557582842952e-09\n",
      "Iteration 5160, loss=1.3553298240653877e-11\n",
      "Iteration 5165, loss=4.378134743632245e-09\n",
      "Iteration 5170, loss=3.842114182361911e-09\n",
      "Iteration 5175, loss=2.858659420290621e-10\n",
      "Iteration 5180, loss=4.073855475184018e-09\n",
      "Iteration 5185, loss=2.8459995471408206e-10\n",
      "Iteration 5190, loss=1.5151478704490684e-13\n",
      "Iteration 5195, loss=1.3181283321783699e-11\n",
      "Iteration 5200, loss=3.9512886296222405e-09\n",
      "Iteration 5205, loss=1.493502451701753e-13\n",
      "Iteration 5210, loss=7.533651036339961e-09\n",
      "Iteration 5215, loss=3.645072021996043e-09\n",
      "Iteration 5220, loss=6.97413593542251e-09\n",
      "Iteration 5225, loss=6.9640870847820224e-09\n",
      "Iteration 5230, loss=1.1946443834176534e-08\n",
      "Iteration 5235, loss=1.1704746505358798e-08\n",
      "Iteration 5240, loss=5.725979781130513e-16\n",
      "Iteration 5245, loss=3.660029612717608e-09\n",
      "Iteration 5250, loss=7.230049448025966e-09\n",
      "Iteration 5255, loss=1.2514254389695001e-11\n",
      "Iteration 5260, loss=3.684333726994282e-09\n",
      "Iteration 5265, loss=3.5782401486272875e-09\n",
      "Iteration 5270, loss=1.4086924985876048e-13\n",
      "Iteration 5275, loss=3.805879167373405e-09\n",
      "Iteration 5280, loss=3.6163263494870534e-09\n",
      "Iteration 5285, loss=7.096999876665677e-09\n",
      "Iteration 5290, loss=1.2294910749222066e-11\n",
      "Iteration 5295, loss=3.611955845528314e-09\n",
      "Iteration 5300, loss=1.2230799706358653e-11\n",
      "Iteration 5305, loss=6.700862087427595e-09\n",
      "Iteration 5310, loss=5.219195204227936e-16\n",
      "Iteration 5315, loss=1.0546179041170944e-08\n",
      "Iteration 5320, loss=3.526522407426569e-09\n",
      "Iteration 5325, loss=6.6274954413358955e-09\n",
      "Iteration 5330, loss=9.99662219669517e-09\n",
      "Iteration 5335, loss=1.1240468191647812e-12\n",
      "Iteration 5340, loss=3.4679772387136154e-09\n",
      "Iteration 5345, loss=1.3280598409187716e-13\n",
      "Iteration 5350, loss=3.433574091715741e-09\n",
      "Iteration 5355, loss=9.47779810189786e-09\n",
      "Iteration 5360, loss=6.457863133135788e-09\n",
      "Iteration 5365, loss=6.419510256705507e-09\n",
      "Iteration 5370, loss=1.0728767874113032e-10\n",
      "Iteration 5375, loss=3.4512757096649693e-09\n",
      "Iteration 5380, loss=1.0678668366237432e-10\n",
      "Iteration 5385, loss=1.0678668366237432e-10\n",
      "Iteration 5390, loss=1.0531645000533274e-12\n",
      "Iteration 5395, loss=1.283250578281575e-13\n",
      "Iteration 5400, loss=2.3910587287900853e-08\n",
      "Iteration 5405, loss=3.34949024072273e-09\n",
      "Iteration 5410, loss=3.3267537613568265e-09\n",
      "Iteration 5415, loss=1.0560600310904888e-10\n",
      "Iteration 5420, loss=3.2756271028944184e-09\n",
      "Iteration 5425, loss=3.2483511436254275e-09\n",
      "Iteration 5430, loss=6.148763720403849e-09\n",
      "Iteration 5435, loss=6.137175212472812e-09\n",
      "Iteration 5440, loss=1.121644357254814e-11\n",
      "Iteration 5445, loss=9.776902594807457e-13\n",
      "Iteration 5450, loss=3.381676494385033e-09\n",
      "Iteration 5455, loss=3.2741842570516155e-09\n",
      "Iteration 5460, loss=1.1056416199251018e-11\n",
      "Iteration 5465, loss=9.512297633601507e-13\n",
      "Iteration 5470, loss=4.1136467426804396e-16\n",
      "Iteration 5475, loss=5.660801605245069e-09\n",
      "Iteration 5480, loss=3.2550682149690147e-09\n",
      "Iteration 5485, loss=3.3516946995604258e-09\n",
      "Iteration 5490, loss=5.820877113649203e-09\n",
      "Iteration 5495, loss=5.479086517823362e-09\n",
      "Iteration 5500, loss=3.085329991492358e-09\n",
      "Iteration 5505, loss=1.0769398393895013e-11\n",
      "Iteration 5510, loss=3.0561799757578e-09\n",
      "Iteration 5515, loss=8.793763243628017e-13\n",
      "Iteration 5520, loss=5.698163718648175e-09\n",
      "Iteration 5525, loss=1.0176778864057212e-10\n",
      "Iteration 5530, loss=2.239467100650927e-08\n",
      "Iteration 5535, loss=7.959018333281165e-09\n",
      "Iteration 5540, loss=3.1530371646937283e-09\n",
      "Iteration 5545, loss=2.9894577924238774e-09\n",
      "Iteration 5550, loss=7.73008146381926e-09\n",
      "Iteration 5555, loss=1.0073978456981436e-10\n",
      "Iteration 5560, loss=2.9604905193991726e-09\n",
      "Iteration 5565, loss=1.1547036384788184e-13\n",
      "Iteration 5570, loss=1.0027853547533994e-10\n",
      "Iteration 5575, loss=3.614279262720055e-16\n",
      "Iteration 5580, loss=3.6100906847958824e-16\n",
      "Iteration 5585, loss=3.602812818894396e-16\n",
      "Iteration 5590, loss=2.916037855626996e-09\n",
      "Iteration 5595, loss=1.1391674279229158e-13\n",
      "Iteration 5600, loss=2.878126403871306e-09\n",
      "Iteration 5605, loss=3.250142155408753e-09\n",
      "Iteration 5610, loss=2.852827085675358e-09\n",
      "Iteration 5615, loss=7.448196726045353e-09\n",
      "Iteration 5620, loss=7.253033285081756e-09\n",
      "Iteration 5625, loss=4.912826589986707e-09\n",
      "Iteration 5630, loss=2.150985523030613e-08\n",
      "Iteration 5635, loss=4.827342525715039e-09\n",
      "Iteration 5640, loss=3.2117437598344623e-09\n",
      "Iteration 5645, loss=1.0113633021335833e-11\n",
      "Iteration 5650, loss=3.017510907810106e-09\n",
      "Iteration 5655, loss=2.1350851753254574e-08\n",
      "Iteration 5660, loss=2.7825772797029913e-09\n",
      "Iteration 5665, loss=3.1969897840156136e-09\n",
      "Iteration 5670, loss=2.1211342016158596e-08\n",
      "Iteration 5675, loss=1.9539590778716587e-10\n",
      "Iteration 5680, loss=6.741410096822165e-09\n",
      "Iteration 5685, loss=6.6577001689438475e-09\n",
      "Iteration 5690, loss=1.9428289532719134e-10\n",
      "Iteration 5695, loss=2.1028217389584825e-08\n",
      "Iteration 5700, loss=5.20589127361859e-09\n",
      "Iteration 5705, loss=1.923346620857913e-10\n",
      "Iteration 5710, loss=2.9129416656559215e-09\n",
      "Iteration 5715, loss=2.9979550615895725e-16\n",
      "Iteration 5720, loss=9.583512455835219e-11\n",
      "Iteration 5725, loss=2.6258877294793592e-09\n",
      "Iteration 5730, loss=6.394092810779739e-09\n",
      "Iteration 5735, loss=3.1389417731730873e-09\n",
      "Iteration 5740, loss=2.8962110487640302e-09\n",
      "Iteration 5745, loss=9.526360950085078e-11\n",
      "Iteration 5750, loss=2.9152213831643047e-16\n",
      "Iteration 5755, loss=1.885922806810214e-10\n",
      "Iteration 5760, loss=1.8817977731622193e-10\n",
      "Iteration 5765, loss=1.0305761777816558e-13\n",
      "Iteration 5770, loss=1.87492618652918e-10\n",
      "Iteration 5775, loss=2.8433801979588225e-09\n",
      "Iteration 5780, loss=1.0229697541526764e-13\n",
      "Iteration 5785, loss=2.8418187802969896e-09\n",
      "Iteration 5790, loss=9.414046625577654e-11\n",
      "Iteration 5795, loss=6.979271761214934e-13\n",
      "Iteration 5800, loss=2.5453839036515546e-09\n",
      "Iteration 5805, loss=5.030894811852704e-09\n",
      "Iteration 5810, loss=6.88348954658996e-13\n",
      "Iteration 5815, loss=2.8134234941745717e-09\n",
      "Iteration 5820, loss=2.707930715751904e-16\n",
      "Iteration 5825, loss=1.8099903520418792e-10\n",
      "Iteration 5830, loss=1.0018910343283921e-13\n",
      "Iteration 5835, loss=3.0659652594522413e-09\n",
      "Iteration 5840, loss=1.7918946881856357e-10\n",
      "Iteration 5845, loss=2.402101406673296e-09\n",
      "Iteration 5850, loss=5.624070542609161e-09\n",
      "Iteration 5855, loss=9.841820148561928e-14\n",
      "Iteration 5860, loss=9.149916067874297e-12\n",
      "Iteration 5865, loss=5.497489130590338e-09\n",
      "Iteration 5870, loss=9.191586380907779e-11\n",
      "Iteration 5875, loss=9.184716875942911e-11\n",
      "Iteration 5880, loss=9.173529297301641e-11\n",
      "Iteration 5885, loss=2.411677524349898e-09\n",
      "Iteration 5890, loss=9.655586741394093e-14\n",
      "Iteration 5895, loss=3.0206666057353004e-09\n",
      "Iteration 5900, loss=2.407886499543809e-16\n",
      "Iteration 5905, loss=1.7340133495746812e-10\n",
      "Iteration 5910, loss=4.756337546041323e-09\n",
      "Iteration 5915, loss=2.6893998139598807e-09\n",
      "Iteration 5920, loss=9.070372924968595e-11\n",
      "Iteration 5925, loss=2.3695971692565666e-16\n",
      "Iteration 5930, loss=2.6711339806695378e-09\n",
      "Iteration 5935, loss=3.864228048655605e-09\n",
      "Iteration 5940, loss=2.3678841110097437e-09\n",
      "Iteration 5945, loss=4.691698141101597e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5950, loss=2.333367187822336e-16\n",
      "Iteration 5955, loss=3.776938761745896e-09\n",
      "Iteration 5960, loss=2.9816675795046876e-09\n",
      "Iteration 5965, loss=8.975427345792042e-11\n",
      "Iteration 5970, loss=2.6411022258088224e-09\n",
      "Iteration 5975, loss=2.9706948012631074e-09\n",
      "Iteration 5980, loss=8.951406976764886e-11\n",
      "Iteration 5985, loss=5.912169083292784e-13\n",
      "Iteration 5990, loss=9.267550106235173e-14\n",
      "Iteration 5995, loss=1.9056164646258367e-08\n",
      "Iteration 6000, loss=3.6557037397244585e-09\n",
      "Iteration 6005, loss=2.6315030154933083e-09\n",
      "Iteration 6010, loss=2.2476868935318074e-16\n",
      "Iteration 6015, loss=1.6313021766745095e-10\n",
      "Iteration 6020, loss=8.88884729710604e-11\n",
      "Iteration 6025, loss=9.132750572503692e-14\n",
      "Iteration 6030, loss=2.208673022252583e-09\n",
      "Iteration 6035, loss=2.1958113105569055e-09\n",
      "Iteration 6040, loss=2.1832493590778768e-09\n",
      "Iteration 6045, loss=1.5996980129440175e-10\n",
      "Iteration 6050, loss=8.818126090437417e-11\n",
      "Iteration 6055, loss=2.165131629539019e-09\n",
      "Iteration 6060, loss=4.43285319562392e-09\n",
      "Iteration 6065, loss=3.4676330695759816e-09\n",
      "Iteration 6070, loss=8.463434814087734e-12\n",
      "Iteration 6075, loss=3.441848805962877e-09\n",
      "Iteration 6080, loss=4.824029176120348e-09\n",
      "Iteration 6085, loss=2.1517680970362107e-09\n",
      "Iteration 6090, loss=4.362505023891572e-09\n",
      "Iteration 6095, loss=2.575108126734449e-09\n",
      "Iteration 6100, loss=2.0990860161163027e-09\n",
      "Iteration 6105, loss=4.768782702058161e-09\n",
      "Iteration 6110, loss=4.340502179900341e-09\n",
      "Iteration 6115, loss=2.0817221280111653e-09\n",
      "Iteration 6120, loss=4.303267520100462e-09\n",
      "Iteration 6125, loss=2.5322803853811138e-09\n",
      "Iteration 6130, loss=2.5320967544928408e-09\n",
      "Iteration 6135, loss=8.696315829991377e-14\n",
      "Iteration 6140, loss=5.206883643867066e-13\n",
      "Iteration 6145, loss=8.647123295180137e-11\n",
      "Iteration 6150, loss=1.526497817039285e-10\n",
      "Iteration 6155, loss=2.5256430280506947e-09\n",
      "Iteration 6160, loss=2.0354042895576185e-09\n",
      "Iteration 6165, loss=4.2685983636658875e-09\n",
      "Iteration 6170, loss=4.525468000338151e-09\n",
      "Iteration 6175, loss=1.8092363163191294e-08\n",
      "Iteration 6180, loss=4.234160133620435e-09\n",
      "Iteration 6185, loss=2.045546620976779e-09\n",
      "Iteration 6190, loss=4.2009293821365645e-09\n",
      "Iteration 6195, loss=2.49928167050939e-09\n",
      "Iteration 6200, loss=1.990287934461321e-09\n",
      "Iteration 6205, loss=1.9792303351806595e-09\n",
      "Iteration 6210, loss=4.388978513958364e-09\n",
      "Iteration 6215, loss=3.1497671137969974e-09\n",
      "Iteration 6220, loss=2.8468478685539367e-09\n",
      "Iteration 6225, loss=3.1082711959840026e-09\n",
      "Iteration 6230, loss=1.9111415129974626e-16\n",
      "Iteration 6235, loss=8.484726116142483e-11\n",
      "Iteration 6240, loss=1.4587965557755211e-10\n",
      "Iteration 6245, loss=7.951707868547597e-12\n",
      "Iteration 6250, loss=7.943037720614665e-12\n",
      "Iteration 6255, loss=4.1050784993501566e-09\n",
      "Iteration 6260, loss=4.1034584619126235e-09\n",
      "Iteration 6265, loss=8.284892953971945e-14\n",
      "Iteration 6270, loss=1.4445591944856062e-10\n",
      "Iteration 6275, loss=2.4374544604910398e-09\n",
      "Iteration 6280, loss=7.891995217057524e-12\n",
      "Iteration 6285, loss=2.4284934063700803e-09\n",
      "Iteration 6290, loss=1.978331942709133e-09\n",
      "Iteration 6295, loss=2.4240092155736193e-09\n",
      "Iteration 6300, loss=2.9458624428713165e-09\n",
      "Iteration 6305, loss=1.9550225882625227e-09\n",
      "Iteration 6310, loss=8.341353996410561e-11\n",
      "Iteration 6315, loss=8.337472379160715e-11\n",
      "Iteration 6320, loss=4.020929367243298e-09\n",
      "Iteration 6325, loss=3.98132193879519e-09\n",
      "Iteration 6330, loss=2.3962531958687805e-09\n",
      "Iteration 6335, loss=3.9494878478762985e-09\n",
      "Iteration 6340, loss=7.983491526284553e-14\n",
      "Iteration 6345, loss=7.970620013618077e-14\n",
      "Iteration 6350, loss=3.94373689260874e-09\n",
      "Iteration 6355, loss=7.947909366236294e-14\n",
      "Iteration 6360, loss=4.4350512303469036e-13\n",
      "Iteration 6365, loss=7.927741850575348e-14\n",
      "Iteration 6370, loss=3.8724623507846445e-09\n",
      "Iteration 6375, loss=2.7674500469032637e-09\n",
      "Iteration 6380, loss=2.7839361926851325e-09\n",
      "Iteration 6385, loss=2.7547317760223677e-09\n",
      "Iteration 6390, loss=1.8532390066994253e-09\n",
      "Iteration 6395, loss=4.3009530398957496e-13\n",
      "Iteration 6400, loss=1.685367263186993e-16\n",
      "Iteration 6405, loss=2.7326585438913753e-09\n",
      "Iteration 6410, loss=2.750946359597606e-09\n",
      "Iteration 6415, loss=2.3456556696999087e-09\n",
      "Iteration 6420, loss=2.700801582378176e-09\n",
      "Iteration 6425, loss=3.781227331245418e-09\n",
      "Iteration 6430, loss=1.6712030870280614e-08\n",
      "Iteration 6435, loss=1.8027696002675953e-09\n",
      "Iteration 6440, loss=2.7327471396887404e-09\n",
      "Iteration 6445, loss=3.7849581246973685e-09\n",
      "Iteration 6450, loss=7.374466488851983e-12\n",
      "Iteration 6455, loss=2.305702739846538e-09\n",
      "Iteration 6460, loss=2.305702739846538e-09\n",
      "Iteration 6465, loss=1.5961609295557223e-16\n",
      "Iteration 6470, loss=1.3149123456734912e-10\n",
      "Iteration 6475, loss=7.348953910690792e-12\n",
      "Iteration 6480, loss=1.3068413018402225e-10\n",
      "Iteration 6485, loss=7.565324912752261e-14\n",
      "Iteration 6490, loss=3.6011766901822284e-09\n",
      "Iteration 6495, loss=1.566596568020791e-16\n",
      "Iteration 6500, loss=1.7442008948265197e-09\n",
      "Iteration 6505, loss=2.587929648356635e-09\n",
      "Iteration 6510, loss=1.6994798901492914e-09\n",
      "Iteration 6515, loss=2.7007243108556622e-09\n",
      "Iteration 6520, loss=7.992050771177261e-11\n",
      "Iteration 6525, loss=7.201315932459096e-12\n",
      "Iteration 6530, loss=2.553601996524435e-09\n",
      "Iteration 6535, loss=2.53994736354457e-09\n",
      "Iteration 6540, loss=3.431112505225542e-09\n",
      "Iteration 6545, loss=1.272474764446585e-10\n",
      "Iteration 6550, loss=2.2484754058638146e-09\n",
      "Iteration 6555, loss=7.126274396973553e-12\n",
      "Iteration 6560, loss=3.8631443421803524e-13\n",
      "Iteration 6565, loss=2.4928497044385267e-09\n",
      "Iteration 6570, loss=1.469134707178445e-16\n",
      "Iteration 6575, loss=1.667343485500794e-09\n",
      "Iteration 6580, loss=1.254394227379052e-10\n",
      "Iteration 6585, loss=2.225618134232832e-09\n",
      "Iteration 6590, loss=1.6570176342156628e-09\n",
      "Iteration 6595, loss=7.021427710085515e-12\n",
      "Iteration 6600, loss=3.254533975649565e-09\n",
      "Iteration 6605, loss=2.4505018014764346e-09\n",
      "Iteration 6610, loss=3.751436552894283e-13\n",
      "Iteration 6615, loss=1.6345140796403257e-09\n",
      "Iteration 6620, loss=3.1857592119877154e-09\n",
      "Iteration 6625, loss=1.6025940574593278e-09\n",
      "Iteration 6630, loss=1.2285142347856493e-10\n",
      "Iteration 6635, loss=2.18551998720784e-09\n",
      "Iteration 6640, loss=7.780381200417352e-11\n",
      "Iteration 6645, loss=1.3539312118602658e-16\n",
      "Iteration 6650, loss=3.1164280045459236e-09\n",
      "Iteration 6655, loss=1.5879887405034765e-09\n",
      "Iteration 6660, loss=7.765910831070144e-11\n",
      "Iteration 6665, loss=2.6338480285659216e-09\n",
      "Iteration 6670, loss=2.632185802653453e-09\n",
      "Iteration 6675, loss=3.616981388877838e-13\n",
      "Iteration 6680, loss=1.2085892009405796e-10\n",
      "Iteration 6685, loss=2.161150147728108e-09\n",
      "Iteration 6690, loss=1.5766711269904476e-09\n",
      "Iteration 6695, loss=1.2042838948289614e-10\n",
      "Iteration 6700, loss=7.727243844790621e-11\n",
      "Iteration 6705, loss=1.3211503750570344e-16\n",
      "Iteration 6710, loss=7.72413383254289e-11\n",
      "Iteration 6715, loss=2.154071809812308e-09\n",
      "Iteration 6720, loss=2.620134109676542e-09\n",
      "Iteration 6725, loss=2.3222754830243275e-09\n",
      "Iteration 6730, loss=2.1490651480604583e-09\n",
      "Iteration 6735, loss=3.4877714050196573e-09\n",
      "Iteration 6740, loss=1.5255178231754485e-09\n",
      "Iteration 6745, loss=2.6111790507599153e-09\n",
      "Iteration 6750, loss=2.304876067782402e-09\n",
      "Iteration 6755, loss=1.1829996704459944e-10\n",
      "Iteration 6760, loss=1.5507948258886017e-09\n",
      "Iteration 6765, loss=3.0089990499249097e-09\n",
      "Iteration 6770, loss=2.1293211638351295e-09\n",
      "Iteration 6775, loss=1.5360140936948596e-09\n",
      "Iteration 6780, loss=1.5290514410182254e-09\n",
      "Iteration 6785, loss=2.2734831794934962e-09\n",
      "Iteration 6790, loss=1.552367123736076e-08\n",
      "Iteration 6795, loss=3.4235814053498737e-13\n",
      "Iteration 6800, loss=1.5478640591481962e-08\n",
      "Iteration 6805, loss=2.9419937597197077e-09\n",
      "Iteration 6810, loss=6.691623302560248e-14\n",
      "Iteration 6815, loss=2.1094850310987567e-09\n",
      "Iteration 6820, loss=1.1650611031477354e-10\n",
      "Iteration 6825, loss=3.386232808912093e-13\n",
      "Iteration 6830, loss=1.5439749034840133e-08\n",
      "Iteration 6835, loss=2.8698492471335157e-09\n",
      "Iteration 6840, loss=1.4622320021473456e-09\n",
      "Iteration 6845, loss=6.56135128868085e-12\n",
      "Iteration 6850, loss=2.83407275425418e-09\n",
      "Iteration 6855, loss=6.614925485625908e-14\n",
      "Iteration 6860, loss=1.4801880832138181e-09\n",
      "Iteration 6865, loss=2.568530055313545e-09\n",
      "Iteration 6870, loss=3.3835412249771935e-09\n",
      "Iteration 6875, loss=6.577270466549129e-14\n",
      "Iteration 6880, loss=6.572517595275496e-14\n",
      "Iteration 6885, loss=1.5268749820052108e-08\n",
      "Iteration 6890, loss=1.446442965402639e-09\n",
      "Iteration 6895, loss=2.0758339491777633e-09\n",
      "Iteration 6900, loss=2.0758339491777633e-09\n",
      "Iteration 6905, loss=1.4357971478418108e-09\n",
      "Iteration 6910, loss=1.1368835040048708e-10\n",
      "Iteration 6915, loss=2.7860398432721922e-09\n",
      "Iteration 6920, loss=1.4581073015662582e-09\n",
      "Iteration 6925, loss=6.496056947566387e-14\n",
      "Iteration 6930, loss=6.481378883030006e-14\n",
      "Iteration 6935, loss=1.4175142171168886e-09\n",
      "Iteration 6940, loss=2.0540462664087045e-09\n",
      "Iteration 6945, loss=6.445331871300294e-14\n",
      "Iteration 6950, loss=6.400989113752864e-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6955, loss=2.5415369808712285e-09\n",
      "Iteration 6960, loss=1.4081847909963585e-09\n",
      "Iteration 6965, loss=2.6979336542609644e-09\n",
      "Iteration 6970, loss=1.505113900179822e-08\n",
      "Iteration 6975, loss=1.1147156114832413e-10\n",
      "Iteration 6980, loss=3.292708994351301e-09\n",
      "Iteration 6985, loss=2.04150074623044e-09\n",
      "Iteration 6990, loss=3.14866161572544e-13\n",
      "Iteration 6995, loss=2.037470636651051e-09\n",
      "Iteration 7000, loss=1.3837798684690483e-09\n",
      "Iteration 7005, loss=1.4978253304320788e-08\n",
      "Iteration 7010, loss=3.2824127860209273e-09\n",
      "Iteration 7015, loss=2.024829637292669e-09\n",
      "Iteration 7020, loss=6.313318059525744e-14\n",
      "Iteration 7025, loss=2.017682021460132e-09\n",
      "Iteration 7030, loss=2.0699431058091022e-09\n",
      "Iteration 7035, loss=3.2544162920089548e-09\n",
      "Iteration 7040, loss=1.361965207280491e-09\n",
      "Iteration 7045, loss=1.3567174050876929e-09\n",
      "Iteration 7050, loss=2.0087886909436747e-09\n",
      "Iteration 7055, loss=1.3425337508365942e-09\n",
      "Iteration 7060, loss=3.0452168022464254e-13\n",
      "Iteration 7065, loss=7.327709966586937e-11\n",
      "Iteration 7070, loss=1.4820646043745e-08\n",
      "Iteration 7075, loss=1.4800336067821718e-08\n",
      "Iteration 7080, loss=1.082686509890074e-10\n",
      "Iteration 7085, loss=2.0286061719332338e-09\n",
      "Iteration 7090, loss=6.2036491395300875e-12\n",
      "Iteration 7095, loss=1.0800740780077218e-16\n",
      "Iteration 7100, loss=2.5290030070124203e-09\n",
      "Iteration 7105, loss=1.0750440121443106e-10\n",
      "Iteration 7110, loss=1.321707077117651e-09\n",
      "Iteration 7115, loss=2.499243700881948e-09\n",
      "Iteration 7120, loss=7.274056357253755e-11\n",
      "Iteration 7125, loss=2.9678513878243773e-13\n",
      "Iteration 7130, loss=7.266276469408695e-11\n",
      "Iteration 7135, loss=1.0681153184144421e-10\n",
      "Iteration 7140, loss=1.9789245797596777e-09\n",
      "Iteration 7145, loss=2.4711159785084647e-09\n",
      "Iteration 7150, loss=1.9758243929857144e-09\n",
      "Iteration 7155, loss=1.0638740582935569e-10\n",
      "Iteration 7160, loss=1.970517526928006e-09\n",
      "Iteration 7165, loss=1.457582499142518e-08\n",
      "Iteration 7170, loss=7.233532522965547e-11\n",
      "Iteration 7175, loss=1.0347975861734683e-16\n",
      "Iteration 7180, loss=1.3488761219093703e-09\n",
      "Iteration 7185, loss=6.084239882742093e-12\n",
      "Iteration 7190, loss=3.159249528650321e-09\n",
      "Iteration 7195, loss=1.9635542081175572e-09\n",
      "Iteration 7200, loss=1.962977558278567e-09\n",
      "Iteration 7205, loss=1.332406185383661e-09\n",
      "Iteration 7210, loss=6.022636680818802e-14\n",
      "Iteration 7215, loss=1.957720430212362e-09\n",
      "Iteration 7220, loss=3.1459426175217686e-09\n",
      "Iteration 7225, loss=1.3173228063934062e-09\n",
      "Iteration 7230, loss=1.9402117690248133e-09\n",
      "Iteration 7235, loss=1.4443209295222914e-08\n",
      "Iteration 7240, loss=2.392143150231618e-09\n",
      "Iteration 7245, loss=1.923288861505057e-09\n",
      "Iteration 7250, loss=1.945559269245223e-09\n",
      "Iteration 7255, loss=1.944999050706997e-09\n",
      "Iteration 7260, loss=7.154674769305203e-11\n",
      "Iteration 7265, loss=7.154674769305203e-11\n",
      "Iteration 7270, loss=2.362267936817375e-09\n",
      "Iteration 7275, loss=1.4347632415478984e-08\n",
      "Iteration 7280, loss=5.9606338961848415e-12\n",
      "Iteration 7285, loss=9.872186867350102e-17\n",
      "Iteration 7290, loss=2.797680164791372e-13\n",
      "Iteration 7295, loss=7.134737245451106e-11\n",
      "Iteration 7300, loss=1.0276961920352434e-10\n",
      "Iteration 7305, loss=1.9319177368970486e-09\n",
      "Iteration 7310, loss=1.930856807774717e-09\n",
      "Iteration 7315, loss=1.0243771109141875e-10\n",
      "Iteration 7320, loss=3.082247790331394e-09\n",
      "Iteration 7325, loss=1.0231820252171175e-10\n",
      "Iteration 7330, loss=1.2254980363834989e-09\n",
      "Iteration 7335, loss=1.2637220159206208e-09\n",
      "Iteration 7340, loss=1.919331138466873e-09\n",
      "Iteration 7345, loss=9.637945199742845e-17\n",
      "Iteration 7350, loss=1.0148323847936069e-10\n",
      "Iteration 7355, loss=1.2132345128534894e-09\n",
      "Iteration 7360, loss=1.9038610687971413e-09\n",
      "Iteration 7365, loss=1.8504169307931306e-09\n",
      "Iteration 7370, loss=1.9031021203375076e-09\n",
      "Iteration 7375, loss=1.414786865439055e-08\n",
      "Iteration 7380, loss=1.1959843115860735e-09\n",
      "Iteration 7385, loss=1.8942549750988746e-09\n",
      "Iteration 7390, loss=5.782227561340614e-12\n",
      "Iteration 7395, loss=5.707079638516896e-14\n",
      "Iteration 7400, loss=3.0334377232321685e-09\n",
      "Iteration 7405, loss=1.836843455116366e-09\n",
      "Iteration 7410, loss=5.690807119160604e-14\n",
      "Iteration 7415, loss=1.0004183592648985e-10\n",
      "Iteration 7420, loss=7.011888986108161e-11\n",
      "Iteration 7425, loss=5.745639641147049e-12\n",
      "Iteration 7430, loss=7.004270080601671e-11\n",
      "Iteration 7435, loss=1.886200751144429e-09\n",
      "Iteration 7440, loss=6.997820378717989e-11\n",
      "Iteration 7445, loss=9.92644369479656e-11\n",
      "Iteration 7450, loss=5.64891185314987e-14\n",
      "Iteration 7455, loss=5.644808147927012e-14\n",
      "Iteration 7460, loss=9.135436286339331e-17\n",
      "Iteration 7465, loss=1.7966492737997442e-09\n",
      "Iteration 7470, loss=2.988676861548356e-09\n",
      "Iteration 7475, loss=2.404209720197059e-09\n",
      "Iteration 7480, loss=1.1618107587096915e-09\n",
      "Iteration 7485, loss=9.01420734274152e-17\n",
      "Iteration 7490, loss=2.9822364577825056e-09\n",
      "Iteration 7495, loss=5.5815774772276575e-14\n",
      "Iteration 7500, loss=8.924461555001966e-17\n",
      "Iteration 7505, loss=1.7766119686513093e-09\n",
      "Iteration 7510, loss=1.8595949224931019e-09\n",
      "Iteration 7515, loss=1.8571207904827247e-09\n",
      "Iteration 7520, loss=1.8570710524912215e-09\n",
      "Iteration 7525, loss=2.113734298703207e-09\n",
      "Iteration 7530, loss=8.749276612664056e-17\n",
      "Iteration 7535, loss=1.1441497749231644e-09\n",
      "Iteration 7540, loss=1.3858539205102716e-08\n",
      "Iteration 7545, loss=9.734543032768883e-11\n",
      "Iteration 7550, loss=1.1364570395855367e-09\n",
      "Iteration 7555, loss=1.1317694559309643e-09\n",
      "Iteration 7560, loss=2.0703341263583752e-09\n",
      "Iteration 7565, loss=5.567845663689841e-12\n",
      "Iteration 7570, loss=2.3773241153435265e-09\n",
      "Iteration 7575, loss=2.931049181142953e-09\n",
      "Iteration 7580, loss=5.55162079501903e-12\n",
      "Iteration 7585, loss=6.866909124658704e-11\n",
      "Iteration 7590, loss=8.506040513948143e-17\n",
      "Iteration 7595, loss=2.374831442608638e-09\n",
      "Iteration 7600, loss=1.7357713044674483e-09\n",
      "Iteration 7605, loss=5.540609204074398e-12\n",
      "Iteration 7610, loss=2.4751348973867104e-13\n",
      "Iteration 7615, loss=1.3707641244309343e-08\n",
      "Iteration 7620, loss=1.1537307775810746e-09\n",
      "Iteration 7625, loss=1.3663005837827313e-08\n",
      "Iteration 7630, loss=1.7071796198919742e-09\n",
      "Iteration 7635, loss=9.488044377947702e-11\n",
      "Iteration 7640, loss=1.36405349238089e-08\n",
      "Iteration 7645, loss=9.476126133778351e-11\n",
      "Iteration 7650, loss=2.8904438842403124e-09\n",
      "Iteration 7655, loss=9.435149883607608e-11\n",
      "Iteration 7660, loss=1.813073247092234e-09\n",
      "Iteration 7665, loss=1.3588029368349908e-08\n",
      "Iteration 7670, loss=1.3587718505903013e-08\n",
      "Iteration 7675, loss=6.804093399814803e-11\n",
      "Iteration 7680, loss=5.484703403252356e-12\n",
      "Iteration 7685, loss=6.796829765676193e-11\n",
      "Iteration 7690, loss=1.8064422180330553e-09\n",
      "Iteration 7695, loss=6.791257833871356e-11\n",
      "Iteration 7700, loss=2.3526052217448523e-09\n",
      "Iteration 7705, loss=5.3379996007175273e-14\n",
      "Iteration 7710, loss=2.8535562801579317e-09\n",
      "Iteration 7715, loss=1.8026458104003495e-09\n",
      "Iteration 7720, loss=2.372101538632154e-13\n",
      "Iteration 7725, loss=1.3472056359375983e-08\n",
      "Iteration 7730, loss=2.8398789986283646e-09\n",
      "Iteration 7735, loss=1.9459860389758887e-09\n",
      "Iteration 7740, loss=1.7922682227222708e-09\n",
      "Iteration 7745, loss=1.7916325090183705e-09\n",
      "Iteration 7750, loss=1.9273540541320244e-09\n",
      "Iteration 7755, loss=1.3379637842092507e-08\n",
      "Iteration 7760, loss=2.8176121436018775e-09\n",
      "Iteration 7765, loss=1.1092695650916085e-09\n",
      "Iteration 7770, loss=1.0639679137725011e-09\n",
      "Iteration 7775, loss=1.1046435988149028e-09\n",
      "Iteration 7780, loss=9.133234446512262e-11\n",
      "Iteration 7785, loss=1.6104783062687034e-09\n",
      "Iteration 7790, loss=2.296875526146963e-13\n",
      "Iteration 7795, loss=1.0505518677206283e-09\n",
      "Iteration 7800, loss=2.277849404323101e-13\n",
      "Iteration 7805, loss=1.7708640109859175e-09\n",
      "Iteration 7810, loss=1.7708640109859175e-09\n",
      "Iteration 7815, loss=1.7698171816959984e-09\n",
      "Iteration 7820, loss=2.3232769041925394e-09\n",
      "Iteration 7825, loss=1.092326007423594e-09\n",
      "Iteration 7830, loss=6.681125097607321e-11\n",
      "Iteration 7835, loss=1.042125941097538e-09\n",
      "Iteration 7840, loss=6.671421748372097e-11\n",
      "Iteration 7845, loss=5.294928991789183e-12\n",
      "Iteration 7850, loss=5.142255029991563e-14\n",
      "Iteration 7855, loss=1.763515999897436e-09\n",
      "Iteration 7860, loss=5.290638586952223e-12\n",
      "Iteration 7865, loss=2.2368528741507215e-13\n",
      "Iteration 7870, loss=1.7592896028872929e-09\n",
      "Iteration 7875, loss=1.317481768126072e-08\n",
      "Iteration 7880, loss=7.589738800454162e-17\n",
      "Iteration 7885, loss=5.272927927624238e-12\n",
      "Iteration 7890, loss=1.5507534145697832e-09\n",
      "Iteration 7895, loss=5.1130124033337346e-14\n",
      "Iteration 7900, loss=5.265199734538761e-12\n",
      "Iteration 7905, loss=1.820110284711518e-09\n",
      "Iteration 7910, loss=2.736424420390904e-09\n",
      "Iteration 7915, loss=1.3093852224699276e-08\n",
      "Iteration 7920, loss=1.7437884469728715e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7925, loss=2.1997514758082676e-13\n",
      "Iteration 7930, loss=2.7340818498089448e-09\n",
      "Iteration 7935, loss=1.8040113847206385e-09\n",
      "Iteration 7940, loss=2.727997605589394e-09\n",
      "Iteration 7945, loss=1.5311643064563896e-09\n",
      "Iteration 7950, loss=1.0186590460037337e-09\n",
      "Iteration 7955, loss=1.061936427682042e-09\n",
      "Iteration 7960, loss=5.036152972513058e-14\n",
      "Iteration 7965, loss=1.3021004718893892e-08\n",
      "Iteration 7970, loss=1.778961866705231e-09\n",
      "Iteration 7975, loss=8.775458831822291e-11\n",
      "Iteration 7980, loss=1.010240446852606e-09\n",
      "Iteration 7985, loss=1.0070007050444474e-09\n",
      "Iteration 7990, loss=5.0103892795761926e-14\n",
      "Iteration 7995, loss=7.261737864775245e-17\n",
      "Iteration 8000, loss=1.727721743449706e-09\n",
      "Iteration 8005, loss=1.046645881075392e-09\n",
      "Iteration 8010, loss=1.725930065532566e-09\n",
      "Iteration 8015, loss=1.5044299139788109e-09\n",
      "Iteration 8020, loss=1.7245183059344527e-09\n",
      "Iteration 8025, loss=4.976331440019813e-14\n",
      "Iteration 8030, loss=1.7245183059344527e-09\n",
      "Iteration 8035, loss=4.971549769625973e-14\n",
      "Iteration 8040, loss=1.7233247051606781e-09\n",
      "Iteration 8045, loss=1.7229369042581766e-09\n",
      "Iteration 8050, loss=1.720949938111005e-09\n",
      "Iteration 8055, loss=1.720949938111005e-09\n",
      "Iteration 8060, loss=2.281267175163748e-09\n",
      "Iteration 8065, loss=2.280766686624247e-09\n",
      "Iteration 8070, loss=7.18631421303368e-17\n",
      "Iteration 8075, loss=1.0225056357171525e-09\n",
      "Iteration 8080, loss=8.644352594844307e-11\n",
      "Iteration 8085, loss=9.853298177375791e-10\n",
      "Iteration 8090, loss=1.753366896117825e-09\n",
      "Iteration 8095, loss=1.0146586904014043e-09\n",
      "Iteration 8100, loss=2.6777655648402288e-09\n",
      "Iteration 8105, loss=1.0096162794681618e-09\n",
      "Iteration 8110, loss=2.0922304687365223e-13\n",
      "Iteration 8115, loss=1.7439214516912216e-09\n",
      "Iteration 8120, loss=6.515399331163962e-11\n",
      "Iteration 8125, loss=7.096389092537325e-17\n",
      "Iteration 8130, loss=4.8882223952200626e-14\n",
      "Iteration 8135, loss=7.096064175992714e-17\n",
      "Iteration 8140, loss=1.001337124328927e-09\n",
      "Iteration 8145, loss=2.665377918376066e-09\n",
      "Iteration 8150, loss=2.0721391182781934e-13\n",
      "Iteration 8155, loss=1.7085705072972246e-09\n",
      "Iteration 8160, loss=8.557163311273541e-11\n",
      "Iteration 8165, loss=9.609600892801495e-10\n",
      "Iteration 8170, loss=2.6594415558633955e-09\n",
      "Iteration 8175, loss=6.960061786654705e-17\n",
      "Iteration 8180, loss=1.2785874581311418e-08\n",
      "Iteration 8185, loss=8.515172594814047e-11\n",
      "Iteration 8190, loss=2.0505073875833058e-13\n",
      "Iteration 8195, loss=2.048646490079506e-13\n",
      "Iteration 8200, loss=1.4641604595411195e-09\n",
      "Iteration 8205, loss=9.52320444724819e-10\n",
      "Iteration 8210, loss=4.994793641188711e-12\n",
      "Iteration 8215, loss=9.456052607603738e-10\n",
      "Iteration 8220, loss=2.2545563194142915e-09\n",
      "Iteration 8225, loss=6.448292594329885e-11\n",
      "Iteration 8230, loss=9.365909159342323e-10\n",
      "Iteration 8235, loss=9.721096150272501e-10\n",
      "Iteration 8240, loss=1.6870586039274826e-09\n",
      "Iteration 8245, loss=4.944961540936932e-12\n",
      "Iteration 8250, loss=9.2687185704321e-10\n",
      "Iteration 8255, loss=4.735261797473092e-14\n",
      "Iteration 8260, loss=2.6272353181866492e-09\n",
      "Iteration 8265, loss=1.6841619210339331e-09\n",
      "Iteration 8270, loss=1.4425306504861624e-09\n",
      "Iteration 8275, loss=9.210754936539445e-10\n",
      "Iteration 8280, loss=1.2668987636743623e-08\n",
      "Iteration 8285, loss=9.186122418292086e-10\n",
      "Iteration 8290, loss=1.436755936445877e-09\n",
      "Iteration 8295, loss=8.361950021296138e-11\n",
      "Iteration 8300, loss=2.615340832790025e-09\n",
      "Iteration 8305, loss=4.904786212595047e-12\n",
      "Iteration 8310, loss=4.898783635687298e-12\n",
      "Iteration 8315, loss=6.737410573790542e-17\n",
      "Iteration 8320, loss=1.4259449176989847e-09\n",
      "Iteration 8325, loss=4.883996419097203e-12\n",
      "Iteration 8330, loss=9.06425490221352e-10\n",
      "Iteration 8335, loss=1.9589005710498458e-13\n",
      "Iteration 8340, loss=4.863999827908749e-12\n",
      "Iteration 8345, loss=6.681307875924745e-17\n",
      "Iteration 8350, loss=1.6687655701730364e-09\n",
      "Iteration 8355, loss=1.2566800045021864e-08\n",
      "Iteration 8360, loss=8.966175024660572e-10\n",
      "Iteration 8365, loss=1.6655888890326764e-09\n",
      "Iteration 8370, loss=1.2557912043575925e-08\n",
      "Iteration 8375, loss=1.4114492907779663e-09\n",
      "Iteration 8380, loss=2.5878605924845033e-09\n",
      "Iteration 8385, loss=2.225792217203093e-09\n",
      "Iteration 8390, loss=4.615769504351212e-14\n",
      "Iteration 8395, loss=6.592293977591179e-17\n",
      "Iteration 8400, loss=4.607958166511633e-14\n",
      "Iteration 8405, loss=4.6054191005489437e-14\n",
      "Iteration 8410, loss=1.6600830710089554e-09\n",
      "Iteration 8415, loss=6.351082854072487e-11\n",
      "Iteration 8420, loss=1.6239734001999295e-09\n",
      "Iteration 8425, loss=6.538447166691937e-17\n",
      "Iteration 8430, loss=1.2511625513411673e-08\n",
      "Iteration 8435, loss=1.399819038461203e-09\n",
      "Iteration 8440, loss=1.6565498972553883e-09\n",
      "Iteration 8445, loss=2.2194754922821858e-09\n",
      "Iteration 8450, loss=1.2483830857945577e-08\n",
      "Iteration 8455, loss=1.6537844427233495e-09\n",
      "Iteration 8460, loss=8.817390706461481e-10\n",
      "Iteration 8465, loss=1.3927667907864816e-09\n",
      "Iteration 8470, loss=9.1584634320796e-10\n",
      "Iteration 8475, loss=1.383758663209278e-09\n",
      "Iteration 8480, loss=4.769420969275018e-12\n",
      "Iteration 8485, loss=1.6006359571107964e-09\n",
      "Iteration 8490, loss=6.305325706001952e-11\n",
      "Iteration 8495, loss=1.241111036165421e-08\n",
      "Iteration 8500, loss=1.6428655102984635e-09\n",
      "Iteration 8505, loss=1.3735402815129305e-09\n",
      "Iteration 8510, loss=1.6413465031561714e-09\n",
      "Iteration 8515, loss=1.2368738033785576e-08\n",
      "Iteration 8520, loss=6.290094139993485e-11\n",
      "Iteration 8525, loss=6.286399872879045e-11\n",
      "Iteration 8530, loss=6.341682074789253e-17\n",
      "Iteration 8535, loss=1.235273039412732e-08\n",
      "Iteration 8540, loss=9.048501947717114e-10\n",
      "Iteration 8545, loss=9.028210956607552e-10\n",
      "Iteration 8550, loss=1.5748317094832487e-09\n",
      "Iteration 8555, loss=1.6361485499771788e-09\n",
      "Iteration 8560, loss=6.31324823754111e-17\n",
      "Iteration 8565, loss=8.999431200251706e-10\n",
      "Iteration 8570, loss=8.95651774968087e-10\n",
      "Iteration 8575, loss=1.6338190800269103e-09\n",
      "Iteration 8580, loss=8.605500201319671e-10\n",
      "Iteration 8585, loss=6.26332954193476e-17\n",
      "Iteration 8590, loss=1.2268552396221821e-08\n",
      "Iteration 8595, loss=1.2260878534675612e-08\n",
      "Iteration 8600, loss=6.2485287777303e-11\n",
      "Iteration 8605, loss=7.91829379842568e-11\n",
      "Iteration 8610, loss=6.208521878035977e-17\n",
      "Iteration 8615, loss=4.4497158101187637e-14\n",
      "Iteration 8620, loss=1.8169607528609188e-13\n",
      "Iteration 8625, loss=1.8160044865447866e-13\n",
      "Iteration 8630, loss=4.44157582349565e-14\n",
      "Iteration 8635, loss=4.4357512861371504e-14\n",
      "Iteration 8640, loss=1.8060905418795792e-13\n",
      "Iteration 8645, loss=4.656263655894444e-12\n",
      "Iteration 8650, loss=1.3176344459964184e-09\n",
      "Iteration 8655, loss=1.7912632640192105e-13\n",
      "Iteration 8660, loss=1.217333256420261e-08\n",
      "Iteration 8665, loss=1.216752920640829e-08\n",
      "Iteration 8670, loss=1.6127621460526598e-09\n",
      "Iteration 8675, loss=1.3094794137913368e-09\n",
      "Iteration 8680, loss=7.809670965475135e-11\n",
      "Iteration 8685, loss=6.203604990817624e-11\n",
      "Iteration 8690, loss=8.662314199270327e-10\n",
      "Iteration 8695, loss=1.3013399247086e-09\n",
      "Iteration 8700, loss=1.2124139914249099e-08\n",
      "Iteration 8705, loss=4.38261013309231e-14\n",
      "Iteration 8710, loss=1.2968129903256909e-09\n",
      "Iteration 8715, loss=4.615774342603407e-12\n",
      "Iteration 8720, loss=1.2929379789028417e-09\n",
      "Iteration 8725, loss=6.023936855217014e-17\n",
      "Iteration 8730, loss=1.605770072465873e-09\n",
      "Iteration 8735, loss=4.355269264807657e-14\n",
      "Iteration 8740, loss=8.29479207542505e-10\n",
      "Iteration 8745, loss=4.338645396184844e-14\n",
      "Iteration 8750, loss=1.4893920541325656e-09\n",
      "Iteration 8755, loss=8.512238136582084e-10\n",
      "Iteration 8760, loss=1.4787517876868606e-09\n",
      "Iteration 8765, loss=2.437812396394179e-09\n",
      "Iteration 8770, loss=5.863508815007049e-17\n",
      "Iteration 8775, loss=1.5890643245697333e-09\n",
      "Iteration 8780, loss=1.5890643245697333e-09\n",
      "Iteration 8785, loss=6.140178643310179e-11\n",
      "Iteration 8790, loss=2.160857492938817e-09\n",
      "Iteration 8795, loss=4.539769168226959e-12\n",
      "Iteration 8800, loss=8.146467389558154e-10\n",
      "Iteration 8805, loss=1.713589800654905e-13\n",
      "Iteration 8810, loss=1.5847905210364388e-09\n",
      "Iteration 8815, loss=6.125065732387469e-11\n",
      "Iteration 8820, loss=6.123068718721925e-11\n",
      "Iteration 8825, loss=4.2683840132101e-14\n",
      "Iteration 8830, loss=2.1537924776993123e-09\n",
      "Iteration 8835, loss=7.594221779205768e-11\n",
      "Iteration 8840, loss=1.5765959648916805e-09\n",
      "Iteration 8845, loss=8.341516366527912e-10\n",
      "Iteration 8850, loss=4.500609954161128e-12\n",
      "Iteration 8855, loss=2.4155399902525687e-09\n",
      "Iteration 8860, loss=2.1496473490145718e-09\n",
      "Iteration 8865, loss=4.2430909317787285e-14\n",
      "Iteration 8870, loss=2.4135735632313526e-09\n",
      "Iteration 8875, loss=4.493841930519604e-12\n",
      "Iteration 8880, loss=2.1477701839245356e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8885, loss=1.1897530960425229e-08\n",
      "Iteration 8890, loss=4.2288872056928106e-14\n",
      "Iteration 8895, loss=2.4021791222850197e-09\n",
      "Iteration 8900, loss=1.5678246478856295e-09\n",
      "Iteration 8905, loss=5.632323745199299e-17\n",
      "Iteration 8910, loss=1.415431216678087e-09\n",
      "Iteration 8915, loss=5.615439996280356e-17\n",
      "Iteration 8920, loss=2.1432893237971484e-09\n",
      "Iteration 8925, loss=4.470682071072707e-12\n",
      "Iteration 8930, loss=8.21788082028263e-10\n",
      "Iteration 8935, loss=8.198515200064094e-10\n",
      "Iteration 8940, loss=1.5643715212121378e-09\n",
      "Iteration 8945, loss=2.140053911858786e-09\n",
      "Iteration 8950, loss=7.476727570399078e-11\n",
      "Iteration 8955, loss=7.475457752814663e-11\n",
      "Iteration 8960, loss=1.6584338611604077e-13\n",
      "Iteration 8965, loss=1.1828497292754037e-08\n",
      "Iteration 8970, loss=4.452522552045313e-12\n",
      "Iteration 8975, loss=2.387858133445775e-09\n",
      "Iteration 8980, loss=2.387858133445775e-09\n",
      "Iteration 8985, loss=1.3987327962539098e-09\n",
      "Iteration 8990, loss=1.6484682813667356e-13\n",
      "Iteration 8995, loss=1.6476478113727072e-13\n",
      "Iteration 9000, loss=1.5552499288418176e-09\n",
      "Iteration 9005, loss=1.2181385899978636e-09\n",
      "Iteration 9010, loss=1.176726804885675e-08\n",
      "Iteration 9015, loss=7.802572477011438e-10\n",
      "Iteration 9020, loss=1.5501737671286264e-09\n",
      "Iteration 9025, loss=7.787318012653088e-10\n",
      "Iteration 9030, loss=1.5498692329529717e-09\n",
      "Iteration 9035, loss=8.083055891283664e-10\n",
      "Iteration 9040, loss=8.060132561382716e-10\n",
      "Iteration 9045, loss=1.5481586013166293e-09\n",
      "Iteration 9050, loss=7.741823293549999e-10\n",
      "Iteration 9055, loss=7.707993687766646e-10\n",
      "Iteration 9060, loss=2.124157294502993e-09\n",
      "Iteration 9065, loss=2.3586796960017864e-09\n",
      "Iteration 9070, loss=2.358256923074009e-09\n",
      "Iteration 9075, loss=1.5460693836288897e-09\n",
      "Iteration 9080, loss=2.358256923074009e-09\n",
      "Iteration 9085, loss=7.682996461255698e-10\n",
      "Iteration 9090, loss=6.007512542982596e-11\n",
      "Iteration 9095, loss=2.3500936752185453e-09\n",
      "Iteration 9100, loss=1.598182984606858e-13\n",
      "Iteration 9105, loss=5.408439040605696e-17\n",
      "Iteration 9110, loss=4.357932851389856e-12\n",
      "Iteration 9115, loss=1.5911356704857021e-13\n",
      "Iteration 9120, loss=1.5391609098358572e-09\n",
      "Iteration 9125, loss=2.1159165530804103e-09\n",
      "Iteration 9130, loss=4.345159215074501e-12\n",
      "Iteration 9135, loss=1.1636741348297619e-08\n",
      "Iteration 9140, loss=7.54896589683085e-10\n",
      "Iteration 9145, loss=1.5363099681309222e-09\n",
      "Iteration 9150, loss=1.578262531515967e-13\n",
      "Iteration 9155, loss=2.3339896682017525e-09\n",
      "Iteration 9160, loss=1.5337248138180826e-09\n",
      "Iteration 9165, loss=1.5334614689166415e-09\n",
      "Iteration 9170, loss=5.967416838448258e-11\n",
      "Iteration 9175, loss=5.30383047161979e-17\n",
      "Iteration 9180, loss=5.966915850308396e-11\n",
      "Iteration 9185, loss=1.1747012251817068e-09\n",
      "Iteration 9190, loss=1.1586069881275307e-08\n",
      "Iteration 9195, loss=1.342910338486547e-09\n",
      "Iteration 9200, loss=2.1074220146743983e-09\n",
      "Iteration 9205, loss=1.5291038435449877e-09\n",
      "Iteration 9210, loss=7.456664175009564e-10\n",
      "Iteration 9215, loss=1.5567988521578147e-13\n",
      "Iteration 9220, loss=7.433149651348003e-10\n",
      "Iteration 9225, loss=1.5245840145894363e-09\n",
      "Iteration 9230, loss=1.5500552501702264e-13\n",
      "Iteration 9235, loss=1.5443804004741657e-13\n",
      "Iteration 9240, loss=1.5222130222980468e-09\n",
      "Iteration 9245, loss=1.316504016912745e-09\n",
      "Iteration 9250, loss=1.151480066852173e-08\n",
      "Iteration 9255, loss=2.3046342612076387e-09\n",
      "Iteration 9260, loss=7.373039956348748e-10\n",
      "Iteration 9265, loss=5.926672347333906e-11\n",
      "Iteration 9270, loss=1.3073108151573365e-09\n",
      "Iteration 9275, loss=1.5136432107709652e-09\n",
      "Iteration 9280, loss=1.1506737784827692e-09\n",
      "Iteration 9285, loss=7.109891148049385e-11\n",
      "Iteration 9290, loss=1.5240518807906056e-13\n",
      "Iteration 9295, loss=3.9687335720312716e-14\n",
      "Iteration 9300, loss=7.290199000031805e-10\n",
      "Iteration 9305, loss=1.5091058402916246e-09\n",
      "Iteration 9310, loss=7.276793057009456e-10\n",
      "Iteration 9315, loss=2.286411726615256e-09\n",
      "Iteration 9320, loss=1.5082972648627901e-09\n",
      "Iteration 9325, loss=7.077162467172826e-11\n",
      "Iteration 9330, loss=7.076055019705763e-11\n",
      "Iteration 9335, loss=1.1432102375863451e-08\n",
      "Iteration 9340, loss=1.137122729311102e-09\n",
      "Iteration 9345, loss=1.2885484901303812e-09\n",
      "Iteration 9350, loss=3.944824881248893e-14\n",
      "Iteration 9355, loss=1.5039622880408388e-09\n",
      "Iteration 9360, loss=7.224587039722508e-10\n",
      "Iteration 9365, loss=2.274112009814644e-09\n",
      "Iteration 9370, loss=5.02507259042289e-17\n",
      "Iteration 9375, loss=1.501065050035777e-09\n",
      "Iteration 9380, loss=5.01938258042526e-17\n",
      "Iteration 9385, loss=5.873992958704832e-11\n",
      "Iteration 9390, loss=1.278108840985226e-09\n",
      "Iteration 9395, loss=1.4864971504147811e-13\n",
      "Iteration 9400, loss=3.917524670545708e-14\n",
      "Iteration 9405, loss=1.4984161689213238e-09\n",
      "Iteration 9410, loss=5.868080327209313e-11\n",
      "Iteration 9415, loss=5.866950675281757e-11\n",
      "Iteration 9420, loss=7.140036339947642e-10\n",
      "Iteration 9425, loss=1.135086336034874e-08\n",
      "Iteration 9430, loss=3.906488847682521e-14\n",
      "Iteration 9435, loss=2.0788135657312523e-09\n",
      "Iteration 9440, loss=4.186221681801161e-12\n",
      "Iteration 9445, loss=1.1163374669109771e-09\n",
      "Iteration 9450, loss=2.076131044859153e-09\n",
      "Iteration 9455, loss=1.1326511284437402e-08\n",
      "Iteration 9460, loss=4.946014968814257e-17\n",
      "Iteration 9465, loss=7.073387431333344e-10\n",
      "Iteration 9470, loss=5.846975681400579e-11\n",
      "Iteration 9475, loss=2.2487798290171668e-09\n",
      "Iteration 9480, loss=2.072432003785707e-09\n",
      "Iteration 9485, loss=1.2495091628039745e-09\n",
      "Iteration 9490, loss=2.23927787423861e-09\n",
      "Iteration 9495, loss=3.864675235647902e-14\n",
      "Iteration 9500, loss=3.859445315418375e-14\n",
      "Iteration 9505, loss=1.2444336672245981e-09\n",
      "Iteration 9510, loss=2.066857796023669e-09\n",
      "Iteration 9515, loss=1.4801712078238438e-09\n",
      "Iteration 9520, loss=1.4482951513419828e-13\n",
      "Iteration 9525, loss=1.4482538161341568e-13\n",
      "Iteration 9530, loss=6.877066971444634e-11\n",
      "Iteration 9535, loss=4.8266144252424325e-17\n",
      "Iteration 9540, loss=3.835441756945904e-14\n",
      "Iteration 9545, loss=5.808272612872756e-11\n",
      "Iteration 9550, loss=5.808272612872756e-11\n",
      "Iteration 9555, loss=3.829250284914654e-14\n",
      "Iteration 9560, loss=6.836315541436377e-11\n",
      "Iteration 9565, loss=4.119897565463271e-12\n",
      "Iteration 9570, loss=6.832991117367015e-11\n",
      "Iteration 9575, loss=4.11980302303383e-12\n",
      "Iteration 9580, loss=1.4731356134944917e-09\n",
      "Iteration 9585, loss=1.1197457183698134e-08\n",
      "Iteration 9590, loss=1.4317496840887678e-13\n",
      "Iteration 9595, loss=4.751403855226661e-17\n",
      "Iteration 9600, loss=3.8214213287897716e-14\n",
      "Iteration 9605, loss=1.4710549445240417e-09\n",
      "Iteration 9610, loss=1.4707941531355573e-09\n",
      "Iteration 9615, loss=2.214519678744864e-09\n",
      "Iteration 9620, loss=2.2139283739619486e-09\n",
      "Iteration 9625, loss=2.0555748214690084e-09\n",
      "Iteration 9630, loss=1.2068460675251913e-09\n",
      "Iteration 9635, loss=1.465962795599296e-09\n",
      "Iteration 9640, loss=1.419158708734422e-13\n",
      "Iteration 9645, loss=1.465694454694244e-09\n",
      "Iteration 9650, loss=3.8004437108180716e-14\n",
      "Iteration 9655, loss=1.1148357792478691e-08\n",
      "Iteration 9660, loss=7.210503860655137e-10\n",
      "Iteration 9665, loss=6.865916724052568e-10\n",
      "Iteration 9670, loss=4.079860147637726e-12\n",
      "Iteration 9675, loss=1.071321253931501e-09\n",
      "Iteration 9680, loss=1.4608158016571338e-09\n",
      "Iteration 9685, loss=4.660854378803951e-17\n",
      "Iteration 9690, loss=4.073514962843472e-12\n",
      "Iteration 9695, loss=7.157066606033879e-10\n",
      "Iteration 9700, loss=5.7591823671154785e-11\n",
      "Iteration 9705, loss=1.0634282343602308e-09\n",
      "Iteration 9710, loss=1.0604305211714404e-09\n",
      "Iteration 9715, loss=1.4550290972081825e-09\n",
      "Iteration 9720, loss=1.1068713057227342e-08\n",
      "Iteration 9725, loss=1.1744367700572411e-09\n",
      "Iteration 9730, loss=5.740275269006112e-11\n",
      "Iteration 9735, loss=1.0515321946513723e-09\n",
      "Iteration 9740, loss=7.117669786893543e-10\n",
      "Iteration 9745, loss=4.049678127560075e-12\n",
      "Iteration 9750, loss=1.447765130002665e-09\n",
      "Iteration 9755, loss=7.098688303841527e-10\n",
      "Iteration 9760, loss=6.645095584900673e-11\n",
      "Iteration 9765, loss=1.1545320255379465e-09\n",
      "Iteration 9770, loss=1.37588684477781e-13\n",
      "Iteration 9775, loss=4.490103786293815e-17\n",
      "Iteration 9780, loss=6.774907856943457e-10\n",
      "Iteration 9785, loss=7.036973781460176e-10\n",
      "Iteration 9790, loss=1.0389543669830914e-09\n",
      "Iteration 9795, loss=1.4403285231168184e-09\n",
      "Iteration 9800, loss=1.0985215403991333e-08\n",
      "Iteration 9805, loss=6.989793743805706e-10\n",
      "Iteration 9810, loss=2.0310182424765344e-09\n",
      "Iteration 9815, loss=1.360153038375972e-13\n",
      "Iteration 9820, loss=1.0325714727699165e-09\n",
      "Iteration 9825, loss=5.6966258099588885e-11\n",
      "Iteration 9830, loss=6.954473108500281e-10\n",
      "Iteration 9835, loss=3.692055342008338e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9840, loss=3.993948834851713e-12\n",
      "Iteration 9845, loss=5.688374077328362e-11\n",
      "Iteration 9850, loss=1.135419203102117e-09\n",
      "Iteration 9855, loss=6.895261028816435e-10\n",
      "Iteration 9860, loss=6.880219172167301e-10\n",
      "Iteration 9865, loss=6.638194993691116e-10\n",
      "Iteration 9870, loss=2.0219041996227816e-09\n",
      "Iteration 9875, loss=1.0223534241404764e-09\n",
      "Iteration 9880, loss=6.589513934507352e-10\n",
      "Iteration 9885, loss=3.962089337172792e-12\n",
      "Iteration 9890, loss=4.38473553463334e-17\n",
      "Iteration 9895, loss=5.664781144165687e-11\n",
      "Iteration 9900, loss=6.570362587332568e-10\n",
      "Iteration 9905, loss=2.132471088600596e-09\n",
      "Iteration 9910, loss=1.3252207220048468e-13\n",
      "Iteration 9915, loss=1.0870682132235743e-08\n",
      "Iteration 9920, loss=1.4249417201739334e-09\n",
      "Iteration 9925, loss=6.766746052377925e-10\n",
      "Iteration 9930, loss=6.476655323162106e-11\n",
      "Iteration 9935, loss=1.4231680278697922e-09\n",
      "Iteration 9940, loss=5.648608317199155e-11\n",
      "Iteration 9945, loss=2.1269344063767903e-09\n",
      "Iteration 9950, loss=6.463302809622817e-11\n",
      "Iteration 9955, loss=3.6221571667613764e-14\n",
      "Iteration 9960, loss=6.717917333531886e-10\n",
      "Iteration 9965, loss=4.337190847385017e-17\n",
      "Iteration 9970, loss=4.335271788363894e-17\n",
      "Iteration 9975, loss=1.0010583473274437e-09\n",
      "Iteration 9980, loss=9.968418313022198e-10\n",
      "Iteration 9985, loss=3.6063708443168085e-14\n",
      "Iteration 9990, loss=3.9162510040435805e-12\n",
      "Iteration 9995, loss=2.0083479324028986e-09\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 5 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    #if cnt % 10000 == 0:\n",
    "    #    sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples = np.array( [6, 7, 8, 11, 12, 13, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(w1, w2):\n",
    "    in_arr1 = np.zeros((1,))\n",
    "    in_arr2 = np.zeros((1,))\n",
    "    in_arr1[0,] = dictionary[w1]\n",
    "    in_arr2[0,] = dictionary[w2]\n",
    "    print(validation_model.predict_on_batch([in_arr1, in_arr2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.39483848]]]\n"
     ]
    }
   ],
   "source": [
    "compare('frog', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = dictionary['dog']\n",
    "in_arr1 = np.zeros((1,))\n",
    "in_arr2 = np.zeros((1,))\n",
    "in_arr3 = np.zeros((1,))\n",
    "in_arr1[0,] = valid\n",
    "in_arr2[0,] = 12\n",
    "in_arr3[0,] = dictionary[\"mat.\"]\n",
    "#x  = sim_cb.get_sim(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.90473485]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_model.predict_on_batch([in_arr1, in_arr2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.91498697]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_model.predict_on_batch([in_arr1, in_arr3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-2.0180428e-02,  3.8796391e-02,  3.9581601e-02, -1.7700054e-02],\n",
       "        [-1.3642136e+00,  1.3702832e+00, -3.4327168e-02,  3.5242200e-01],\n",
       "        [ 1.6238487e+00, -1.6949019e+00, -6.9046557e-01,  2.4016201e-01],\n",
       "        [-1.1019892e+00,  1.1608373e+00,  1.1630919e+00, -1.0825593e+00],\n",
       "        [-9.2466658e-01,  9.9118060e-01,  9.4729191e-01, -9.7021735e-01],\n",
       "        [ 2.5630962e-02,  4.9725462e-02, -1.7837264e-02,  4.6070624e-02],\n",
       "        [-1.3387740e-02,  4.9413089e-02, -4.6420097e-03, -3.5513185e-02],\n",
       "        [ 7.5155848e-01, -7.2349083e-01, -7.9123420e-01,  7.6966560e-01],\n",
       "        [ 3.4375515e-02, -1.4950313e-02, -8.1053749e-03, -3.0967152e-02],\n",
       "        [ 1.5945641e+00, -1.5981613e+00,  1.3569762e+00, -1.4269550e+00],\n",
       "        [ 7.9121387e-01, -8.3454752e-01,  8.4425145e-01, -8.0594784e-01],\n",
       "        [ 3.0069660e-02, -2.2222495e-02,  1.3444815e-02, -4.7114935e-02],\n",
       "        [ 4.3911446e-02, -2.2283113e-02,  1.9387674e-02, -2.5411725e-02],\n",
       "        [-1.4537644e-02,  6.6388957e-03, -4.1202068e-02, -1.3308413e-03],\n",
       "        [-2.2272682e-02, -3.3635810e-02, -2.1992255e-02,  1.6817059e-02],\n",
       "        [-4.1599490e-02,  5.0714724e-03, -7.5611360e-03,  8.4341653e-03],\n",
       "        [ 1.6937408e+00, -1.6223340e+00, -1.7076595e+00,  1.5797282e+00],\n",
       "        [ 4.1811649e-02,  4.4170026e-02,  4.4995930e-02, -3.6094476e-02],\n",
       "        [-3.5203695e-02, -3.2173790e-02,  3.1413626e-02, -4.1490603e-02],\n",
       "        [ 1.2511775e+00, -1.2975556e+00, -1.2872654e+00,  1.2049205e+00],\n",
       "        [ 7.9798633e-01, -7.1423870e-01, -7.8376436e-01,  7.4242705e-01],\n",
       "        [ 1.1660408e+00, -1.2041683e+00, -1.0973312e+00,  9.4681376e-01],\n",
       "        [ 4.0364970e-02, -2.8689576e-02, -3.7119962e-02,  1.7366532e-02],\n",
       "        [-1.2578480e-03,  2.3223285e-02,  6.4584501e-03, -2.0874405e-02],\n",
       "        [ 1.9672599e-02, -1.9806623e-03,  2.9158678e-02, -1.9691193e-02]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
